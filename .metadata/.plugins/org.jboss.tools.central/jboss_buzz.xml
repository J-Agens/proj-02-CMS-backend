<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Infinispan 11.0.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/HIk81kizi4A/" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Tristan Tarrant</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_11_0_0_final</id><updated>2020-06-15T16:48:46Z</updated><published>2020-06-15T12:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Dear Infinispan community,&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We’re proud to announce the release of Infinispan 11. In the tradition of assigning beer codenames to our releases, we decided that "Corona Extra" would be a significant representation of the period during which most of the development has happened. We hope that you, your families and friends have not been impacted by the pandemic.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_but_didnt_you_release_10_x_not_long_ago"&gt;&lt;a class="anchor" href="#_but_didnt_you_release_10_x_not_long_ago" /&gt;But didn’t you release 10.x not long ago ?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Indeed, but version numbers are just that: numbers. We are still continuing our near-quarterly releases, but, from now on, these will be identified by major version numbers.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_so_whats_new_in_infinispan_11"&gt;&lt;a class="anchor" href="#_so_whats_new_in_infinispan_11" /&gt;So, what’s new in Infinispan 11 ?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As usual we added new features, improved existing ones and prepared the groundwork for upcoming features.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_conflict_detection_and_resolution_for_asynchronous_cross_site_replication"&gt;&lt;a class="anchor" href="#_conflict_detection_and_resolution_for_asynchronous_cross_site_replication" /&gt;Conflict detection and resolution for Asynchronous Cross-Site Replication&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Cross-site replication is one of our most used features, as it enables a number of very useful use-cases such as geographical load distribution, zero-downtime disaster recovery and follow-the-sun data centers.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In this release we completely overhauled the way we implement asynchronous cross-site replication by introducing conflict resolution, based on vector clocks, as well as multiple site masters to increase throughput and reliability. This means that you can have multiple active sites safely replicating data between each other.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_server_security_overhaul"&gt;&lt;a class="anchor" href="#_server_security_overhaul" /&gt;Server security overhaul&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Infinispan Server’s security, while very powerful, was also tricky to set up because of the configuration complexity. Since we wanted to make the &lt;a href="//infinispan.org/blog/2020/06/04/server-secure-by-default/"&gt;server secure by default&lt;/a&gt;, we put a lot of work in simplifying the configuration and removing all of the boilerplate. Additionally, if you are securing the server with &lt;a href="https://keycloak.org"&gt;Keycloak&lt;/a&gt;, accessing the console will correctly obtain credentials through the realm login page.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_non_blocking_internals"&gt;&lt;a class="anchor" href="#_non_blocking_internals" /&gt;Non-blocking internals&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Our quest to make better use of the available hardware resources in all deployment models (bare-metal, containerized, virtualized) continues as we’ve now consolidated a lot of thread-pools into just two: non-blocking and blocking. Most of the code now makes use of the non-blocking pool. Paths which may block, such as certain persistent stores, use the blocking pool so that they don’t hold up work that may be processed without blocking. This release also includes a new non-blocking Store SPI, so that you can take advantage of stores with real non-blocking I/O.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_clustering"&gt;&lt;a class="anchor" href="#_clustering" /&gt;Clustering&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As Infinispan is participating in &lt;a href="https://cloudbutton.eu/"&gt;CloudButton&lt;/a&gt;, a Serverless Data Analytics Platform which is part of the &lt;a href="https://ec.europa.eu/programmes/horizon2020/"&gt;European Union’s Horizon 2020 research and innovation programme&lt;/a&gt;, we have introduced a new optional feature which allows scaling by adding new nodes to a cluster without state-transfer. This means that you can add capacity with zero-impact to your operations. Obviously this comes at the cost of reduced resilience in case of failures, but, for scenarios where high availability is not required, this gives you a highly scalable in-memory storage solution.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If high availability is your thing, the rebalancing algorithm which decides how segments (our subdivision of the data space) are mapped to nodes has been overhauled to be much more accurate and fairer.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_queryindexing"&gt;&lt;a class="anchor" href="#_queryindexing" /&gt;Query/Indexing&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Querying and indexing will be the major focus in Infinispan 12 (with the long awaited upgrade to &lt;a href="https://hibernate.org/search/"&gt;Hibernate Search 6&lt;/a&gt; and &lt;a href="https://lucene.apache.org/"&gt;Lucene 8&lt;/a&gt;). In preparation for that, &lt;strong&gt;a lot&lt;/strong&gt; of work has gone into deprecations, usability, clean ups and documentation.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_hot_rod_client_improvements"&gt;&lt;a class="anchor" href="#_hot_rod_client_improvements" /&gt;Hot Rod Client improvements&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Many usability changes have been added to our Java Hot Rod client:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;a Hot Rod URI as a compact way to configure a connection&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;automatic creation of caches on demand using supplied configurations/templates with support for wildcards&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;improved iteration of entries by concurrently splitting work across segments/nodes&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_other_server_changes"&gt;&lt;a class="anchor" href="#_other_server_changes" /&gt;Other Server changes&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you are using the JDBC cache store to persist your cache entries to a database, Infinispan Server now restores the ability to create shared datasources which was lost when we abandoned the WildFly base.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_cli"&gt;&lt;a class="anchor" href="#_cli" /&gt;CLI&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The CLI received a number of new features such as logging manipulation, obtaining sever reports and user management, superseding the &lt;code&gt;user-tool&lt;/code&gt; script.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;a class="image" href="//infinispan.org/blog/img/ispn110cli.png"&gt;&lt;img src="//infinispan.org/blog/thumb/ispn101welcome.png" alt="CLI" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_console"&gt;&lt;a class="anchor" href="#_console" /&gt;Console&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Our console overhaul, which started in 10, continues with lots of new features, integrations and polishing. Highlights are:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;entry creation dialog box&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;querying&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;KeyCloak integration&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;a class="image" href="//infinispan.org/blog/img/ispn110console.png"&gt;&lt;img src="//infinispan.org/blog/thumb/ispn110console.png" alt="onsole" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_clouds_containers_and_operators"&gt;&lt;a class="anchor" href="#_clouds_containers_and_operators" /&gt;Clouds, containers and operators&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Our Infinispan Server image is now based on &lt;code&gt;ubi-minimal:8.2&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;And thanks to our friends over at &lt;a href="https://quarkus.io"&gt;Quarkus&lt;/a&gt;, Infinispan Server is now also available as a native image built using &lt;a href="https://graalvm.org"&gt;GraalVM&lt;/a&gt;. This image is available on Quay.io and Docker Hub.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Kubernetes Operator adds a new Cache Custom Resource and the ability to expose services via Ingress and Routes.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_documentation"&gt;&lt;a class="anchor" href="#_documentation" /&gt;Documentation&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Documentation has also received a lot of love in all areas:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Added procedural content for rolling upgrades, Cache CR with the Operator, server patching, misc CLI commands, using RemoteCacheConfigurationBuilder.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Procedural content for different upgrade and migration tasks included in Upgrade Guide.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Operator and Spring Boot Starter guides now provide stable and development versions from the index page.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Updated index.html and throughout documentation to improve high-level context and aid retrievability.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Getting Started content updated and streamlined.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Applied several modifications, additions, and removals to documentation via community feedback.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_whats_next"&gt;&lt;a class="anchor" href="#_whats_next" /&gt;What’s next ?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;As briefly mentioned above, Infinispan 12 will be our next release, scheduled for this autumn. We will be working on query/index improvements, backup/restore capabilities as well as the usual load of improvements, clean-ups across the board. We will keep you posted with development release and blogs about upcoming highlights. If you’d like to contribute, just get in touch.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_get_it_use_it_ask_us"&gt;&lt;a class="anchor" href="#_get_it_use_it_ask_us" /&gt;Get it, Use it, Ask us!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Please &lt;a href="https://infinispan.org/download/"&gt;download&lt;/a&gt;, &lt;a href="https://issues.jboss.org/projects/ISPN"&gt;report bugs&lt;/a&gt;, &lt;a href="https://infinispan.zulipchat.com/"&gt;chat with us&lt;/a&gt;, ask questions on &lt;a href="https://stackoverflow.com/questions/tagged/?tagnames=infinispan&amp;amp;sort=newest"&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/HIk81kizi4A" height="1" width="1" alt=""/&gt;</content><summary>Dear Infinispan community, We’re proud to announce the release of Infinispan 11. In the tradition of assigning beer codenames to our releases, we decided that "Corona Extra" would be a significant representation of the period during which most of the development has happened. We hope that you, your families and friends have not been impacted by the pandemic. But didn’t you release 10.x not long ag...</summary><dc:creator>Tristan Tarrant</dc:creator><dc:date>2020-06-15T12:00:00Z</dc:date><feedburner:origLink>http://infinispan.org/blog/2020/06/15/infinispan-11/</feedburner:origLink></entry><entry><title>Jakarta EE: Multitenancy with JPA on WildFly, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7zZq2y1lyIQ/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Hibernate JPA" scheme="searchisko:content:tags" /><category term="jakarta ee" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="multi-tenancy" scheme="searchisko:content:tags" /><category term="wildfly" scheme="searchisko:content:tags" /><author><name>rhsilva</name></author><id>searchisko:content:id:jbossorg_blog-jakarta_ee_multitenancy_with_jpa_on_wildfly_part_1</id><updated>2020-06-15T07:00:45Z</updated><published>2020-06-15T07:00:45Z</published><content type="html">&lt;p&gt;In this two-part series, I demonstrate two approaches to multitenancy with the &lt;a target="_blank" rel="nofollow" href="https://projects.eclipse.org/projects/ee4j.jpa"&gt;Jakarta Persistence API (JPA)&lt;/a&gt; running on &lt;a target="_blank" rel="nofollow" href="https://wildfly.org"&gt;WildFly&lt;/a&gt;. In the first half of this series, you will learn how to implement multitenancy using a database. In the second half, I will introduce you to multitenancy using a schema. I based both examples on JPA and &lt;a target="_blank" rel="nofollow" href="http://hibernate.org"&gt;Hibernate&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because I have focused on implementation examples, I won&amp;#8217;t go deeply into the details of multitenancy, though I will start with a brief overview. Note, too, that I assume you are familiar with Java persistence using JPA and Hibernate.&lt;/p&gt; &lt;h2&gt;Multitenancy architecture&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://docs.jboss.org/hibernate/orm/4.1/devguide/en-US/html/ch16.html"&gt;Multitenancy&lt;/a&gt; is an architecture that permits a single application to serve multiple tenants, also known as clients. Although tenants in a multitenancy architecture access the same application, they are securely isolated from each other. Furthermore, each tenant only has access to its own resources. Multitenancy is a common architectural approach for software-as-a-service (SaaS) and cloud computing applications. In general, clients (or tenants) accessing a SaaS are accessing the same application, but each one is isolated from the others and has its own resources.&lt;/p&gt; &lt;p&gt;A multitenant architecture must isolate the data available to each tenant. If there is a problem with one tenant&amp;#8217;s data set, it won&amp;#8217;t impact the other tenants. In a relational database, we use a database or a schema to isolate each tenant&amp;#8217;s data. One way to separate data is to give each tenant access to its own database or schema. Another option, which is available if you are using a relational database with JPA and Hibernate, is to partition a single database for multiple tenants. In this article, I focus on the standalone database and schema options. I won&amp;#8217;t demonstrate how to set up a partition.&lt;/p&gt; &lt;p&gt;In a server-based application like WildFly, multitenancy is different from the conventional approach. In this case, the server application works directly with the data source by initiating a connection and preparing the database to be used. The client application does not spend time opening the connection, which improves performance. On the other hand, using Enterprise JavaBeans (EJBs) for container-managed transactions can lead to problems. As an example, the server-based application could do something to generate an error to commit or roll the application back.&lt;/p&gt; &lt;h2&gt;Implementation code&lt;/h2&gt; &lt;p&gt;Two interfaces are crucial to implementing multitenancy in JPA and Hibernate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MultiTenantConnectionProvider&lt;/strong&gt; is responsible for connecting tenants to their respective databases and services. We will use this interface and a tenant identifier to switch between databases for different tenants.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CurrentTenantIdentifierResolver&lt;/strong&gt; is responsible for identifying the tenant. We will use this interface to define what is considered a tenant (more about this later). We will also use this interface to provide the correct tenant identifier to &lt;code&gt;MultiTenantConnectionProvider&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In JPA, we configure these interfaces using the &lt;code&gt;persistence.xml&lt;/code&gt; file. In the next sections, I&amp;#8217;ll show you how to use these two interfaces to create the first three classes we need for our multitenancy architecture: &lt;code&gt;DatabaseMultiTenantProvider&lt;/code&gt;, &lt;code&gt;MultiTenantResolver&lt;/code&gt;, and &lt;code&gt;DatabaseTenantResolver&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;DatabaseMultiTenantProvider&lt;/h3&gt; &lt;p&gt;&lt;code&gt;DatabaseMultiTenantProvider&lt;/code&gt; is an implementation of the &lt;code&gt;MultiTenantConnectionProvider&lt;/code&gt; interface. This class contains logic to switch to the database that matches the given tenant identifier. In WildFly, this means switching to different &lt;em&gt;data sources&lt;/em&gt;. The &lt;code&gt;DatabaseMultiTenantProvider&lt;/code&gt; class also implements the &lt;code&gt;ServiceRegistryAwareService&lt;/code&gt;, which allows us to inject a service during the configuration phase.&lt;/p&gt; &lt;p&gt;Here&amp;#8217;s the code for the &lt;code&gt;DatabaseMultiTenantProvider&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt;public class DatabaseMultiTenantProvider implements MultiTenantConnectionProvider, ServiceRegistryAwareService{ private static final long serialVersionUID = 1L; private static final String TENANT_SUPPORTED = "DATABASE"; private DataSource dataSource; private String typeTenancy ; @Override public boolean supportsAggressiveRelease() { return false; } @Override public void injectServices(ServiceRegistryImplementor serviceRegistry) { typeTenancy = (String) ((ConfigurationService)serviceRegistry .getService(ConfigurationService.class)) .getSettings().get("hibernate.multiTenancy"); dataSource = (DataSource) ((ConfigurationService)serviceRegistry .getService(ConfigurationService.class)) .getSettings().get("hibernate.connection.datasource"); } @SuppressWarnings("rawtypes") @Override public boolean isUnwrappableAs(Class clazz) { return false; } @Override public &amp;#60;T&amp;#62; T unwrap(Class&amp;#60;T&amp;#62; clazz) { return null; } @Override public Connection getAnyConnection() throws SQLException { final Connection connection = dataSource.getConnection(); return connection; } @Override public Connection getConnection(String tenantIdentifier) throws SQLException { final Context init; //Just use the multi-tenancy if the hibernate.multiTenancy == DATABASE &lt;strong&gt;if(TENANT_SUPPORTED.equals(typeTenancy)) {&lt;/strong&gt; try { init = new InitialContext(); &lt;strong&gt; dataSource = (DataSource) init.lookup("java:/jdbc/" + tenantIdentifier);&lt;/strong&gt; } catch (NamingException e) { throw new HibernateException("Error trying to get datasource ['java:/jdbc/" + tenantIdentifier + "']", e); } } return dataSource.getConnection(); } @Override public void releaseAnyConnection(Connection connection) throws SQLException { connection.close(); } @Override public void releaseConnection(String tenantIdentifier, Connection connection) throws SQLException { releaseAnyConnection(connection); } } &lt;/pre&gt; &lt;p&gt;As you can see, we call the &lt;code&gt;injectServices&lt;/code&gt; method to populate the &lt;code&gt;datasource&lt;/code&gt; and &lt;code&gt;typeTenancy&lt;/code&gt; attributes. We use the &lt;code&gt;datasource&lt;/code&gt; attribute to get a connection from the data source, and we use the &lt;code&gt;typeTenancy&lt;/code&gt; attribute to find out if the class supports the &lt;code&gt;multiTenancy&lt;/code&gt; type. We call the &lt;code&gt;getConnection&lt;/code&gt; method to get a data source connection. This method uses the tenant identifier to locate and switch to the correct data source.&lt;/p&gt; &lt;h3&gt;MultiTenantResolver&lt;/h3&gt; &lt;p&gt;&lt;code&gt;MultiTenantResolver&lt;/code&gt; is an abstract class that implements the &lt;code&gt;CurrentTenantIdentifierResolver&lt;/code&gt; interface. This class aims to provide a &lt;code&gt;setTenantIdentifier&lt;/code&gt; method to all &lt;code&gt;CurrentTenantIdentifierResolver&lt;/code&gt; implementations:&lt;/p&gt; &lt;pre&gt;public abstract class MultiTenantResolver implements CurrentTenantIdentifierResolver { &lt;strong&gt;protected String tenantIdentifier; public void setTenantIdentifier(String tenantIdentifier) { this.tenantIdentifier = tenantIdentifier; } &lt;/strong&gt;} &lt;/pre&gt; &lt;p&gt;This abstract class is simple. We only use it to provide the &lt;code&gt;setTenantIdentifier&lt;/code&gt; method.&lt;/p&gt; &lt;h3&gt;DatabaseTenantResolver&lt;/h3&gt; &lt;p&gt;&lt;code&gt;DatabaseTenantResolver&lt;/code&gt; also implements the &lt;code&gt;CurrentTenantIdentifierResolver&lt;/code&gt; interface. This class is the concrete class of &lt;code&gt;MultiTenantResolver&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;public class DatabaseTenantResolver extends MuiltiTenantResolver { private Map&amp;#60;String, String&amp;#62; regionDatasourceMap; public DatabaseTenantResolver(){ &lt;strong&gt;regionDatasourceMap = new HashMap();&lt;/strong&gt; &lt;strong&gt; regionDatasourceMap.put("default", "MyDataSource");&lt;/strong&gt; &lt;strong&gt; regionDatasourceMap.put("america", "AmericaDB");&lt;/strong&gt; &lt;strong&gt; regionDatasourceMap.put("europa", "EuropaDB");&lt;/strong&gt; &lt;strong&gt; regionDatasourceMap.put("asia", "AsiaDB");&lt;/strong&gt; } @Override public String resolveCurrentTenantIdentifier() { &lt;strong&gt;if(this.tenantIdentifier != null&lt;/strong&gt; &lt;strong&gt; &amp;#38;&amp;#38; regionDatasourceMap.containsKey(this.tenantIdentifier)){&lt;/strong&gt; &lt;strong&gt; return regionDatasourceMap.get(this.tenantIdentifier);&lt;/strong&gt; &lt;strong&gt; }&lt;/strong&gt; &lt;strong&gt; return regionDatasourceMap.get("default");&lt;/strong&gt; } @Override public boolean validateExistingCurrentSessions() { return false; } }&lt;/pre&gt; &lt;p&gt;Notice that &lt;code&gt;DatabaseTenantResolver&lt;/code&gt; uses a &lt;code&gt;Map&lt;/code&gt; to define the correct data source for a given tenant. The tenant, in this case, is a region. Note, too, that this example assumes we have the data sources &lt;code&gt;java:/jdbc/MyDataSource&lt;/code&gt;, &lt;code&gt;java:/jdbc/AmericaDB&lt;/code&gt;, &lt;code&gt;java:/jdbc/EuropaDB&lt;/code&gt;, and &lt;code&gt;java:/jdbc/AsiaDB&lt;/code&gt; configured in WildFly.&lt;/p&gt; &lt;h2&gt;Configure and define the tenant&lt;/h2&gt; &lt;p&gt;Now we need to use the &lt;code&gt;persistence.xml&lt;/code&gt; file to configure the tenant:&lt;/p&gt; &lt;pre&gt;&amp;#60;persistence&amp;#62; &amp;#60;persistence-unit name="jakartaee8"&amp;#62; &amp;#60;jta-data-source&amp;#62;jdbc/MyDataSource&amp;#60;/jta-data-source&amp;#62; &amp;#60;properties&amp;#62; &amp;#60;property name="javax.persistence.schema-generation.database.action" value="none" /&amp;#62; &amp;#60;property name="hibernate.dialect" value="org.hibernate.dialect.PostgresPlusDialect"/&amp;#62; &lt;strong&gt;&amp;#60;property name="hibernate.multiTenancy" value="DATABASE"/&amp;#62;&lt;/strong&gt; &lt;strong&gt;&amp;#60;property name="hibernate.tenant_identifier_resolver" value="net.rhuanrocha.dao.multitenancy.DatabaseTenantResolver"/&amp;#62;&lt;/strong&gt; &lt;strong&gt; &amp;#60;property name="hibernate.multi_tenant_connection_provider" value="net.rhuanrocha.dao.multitenancy.DatabaseMultiTenantProvider"/&amp;#62;&lt;/strong&gt; &amp;#60;/properties&amp;#62; &amp;#60;/persistence-unit&amp;#62; &amp;#60;/persistence&amp;#62; &lt;/pre&gt; &lt;p&gt;Next, we define the tenant in the &lt;code&gt;EntityManagerFactory&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;@PersistenceUnit protected EntityManagerFactory emf; protected EntityManager getEntityManager(String multitenancyIdentifier){ final MuiltiTenantResolver tenantResolver = (&lt;strong&gt;MuiltiTenantResolver&lt;/strong&gt;) ((SessionFactoryImplementor) emf).getCurrentTenantIdentifierResolver(); &lt;strong&gt; tenantResolver.setTenantIdentifier(multitenancyIdentifier);&lt;/strong&gt; return emf.createEntityManager(); } &lt;/pre&gt; &lt;p&gt;Note that we call the &lt;code&gt;setTenantIdentifier&lt;/code&gt; before creating a new instance of &lt;code&gt;EntityManager&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;I have presented a simple example of multitenancy in a database using JPA with Hibernate and WildFly. There are many ways to use a database for multitenancy. My main point has been to show you how to implement the &lt;code&gt;CurrentTenantIdentifierResolver&lt;/code&gt; and &lt;code&gt;MultiTenantConnectionProvider&lt;/code&gt; interfaces. I&amp;#8217;ve shown you how to use JPA&amp;#8217;s &lt;code&gt;persistence.xml&lt;/code&gt; file to configure the required classes based on these interfaces.&lt;/p&gt; &lt;p&gt;Keep in mind that for this example, I have assumed that WildFly manages the data source and connection pool and that EJB handles the container-managed transactions. In the second half of this series, I will provide a similar introduction to multitenancy, but using a schema rather than a database. If you want to go deeper with this example, you can &lt;a target="_blank" rel="nofollow" href="https://github.com/rhuan080/multitenancyJpaJakartaEE"&gt;find the complete application code and further instructions&lt;/a&gt; on my GitHub repository.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#38;linkname=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fjakarta-ee-multitenancy-with-jpa-on-wildfly-part-1%2F&amp;#038;title=Jakarta%20EE%3A%20Multitenancy%20with%20JPA%20on%20WildFly%2C%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2020/06/15/jakarta-ee-multitenancy-with-jpa-on-wildfly-part-1/" data-a2a-title="Jakarta EE: Multitenancy with JPA on WildFly, Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/15/jakarta-ee-multitenancy-with-jpa-on-wildfly-part-1/"&gt;Jakarta EE: Multitenancy with JPA on WildFly, Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7zZq2y1lyIQ" height="1" width="1" alt=""/&gt;</content><summary>In this two-part series, I demonstrate two approaches to multitenancy with the Jakarta Persistence API (JPA) running on WildFly. In the first half of this series, you will learn how to implement multitenancy using a database. In the second half, I will introduce you to multitenancy using a schema. I based both examples on JPA and Hibernate. Because I have focused on implementation examples, I won’...</summary><dc:creator>rhsilva</dc:creator><dc:date>2020-06-15T07:00:45Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/15/jakarta-ee-multitenancy-with-jpa-on-wildfly-part-1/</feedburner:origLink></entry><entry><title>Tracking COVID-19 using Quarkus, AMQ Streams, and Camel K on OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QkUhNGsv0ds/" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="COVID-19" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="kubernetes-native" scheme="searchisko:content:tags" /><category term="mongodb" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="Stream Processing" scheme="searchisko:content:tags" /><author><name>gmccarth</name></author><id>searchisko:content:id:jbossorg_blog-tracking_covid_19_using_quarkus_amq_streams_and_camel_k_on_openshift</id><updated>2020-06-15T07:00:04Z</updated><published>2020-06-15T07:00:04Z</published><content type="html">&lt;p&gt;In just a matter of weeks, the world that we knew changed forever. The COVID-19 pandemic came swiftly and caused massive disruption to our healthcare systems and local businesses, throwing the world&amp;#8217;s economies into chaos. The coronavirus quickly became a crisis that affected everyone. As researchers and scientists rushed to make sense of it, and find ways to eliminate or slow the rate of infection, countries started gathering statistics such as the number of confirmed cases, reported deaths, and so on. Johns Hopkins University researchers have since aggregated the &lt;a target="_blank" rel="nofollow" href="https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports"&gt;statistics from many countries&lt;/a&gt; and made them available.&lt;/p&gt; &lt;p&gt;In this article, we demonstrate how to build a website that shows a series of COVID-19 graphs. These graphs reflect the accumulated number of cases and deaths over a given time period for each country. We use the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus, &lt;/a&gt;&lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/index.html#"&gt;Apache Camel K&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ Streams&lt;/a&gt; to get the Johns Hopkins University data and populate a MongoDB database with it. The deployment is built on the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift Container Platform (OCP)&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-729337"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;The two applications developed for this demo would work for many other scenarios, such as reporting on corporate sales numbers, reporting on data from Internet-of-Things (IoT) connected devices, or keeping track of expenses or inventory. Wherever there is a repository with useful data, you could make minor code modifications and use these applications to collect, transform, and present the data to its users in a more meaningful way.&lt;/p&gt; &lt;h2&gt;Technologies we&amp;#8217;ll use&lt;/h2&gt; &lt;p&gt;Our focus in this article is the next-generation &lt;a href="https://developers.redhat.com/blog/2020/04/08/why-kubernetes-native-instead-of-cloud-native/"&gt;Kubernetes-native&lt;/a&gt; &lt;a target="_blank" rel="nofollow" href="http://developers.redhat.com/blog/2020/04/24/ramp-up-on-quarkus-a-kubernetes-native-java-framework/"&gt;Java framework&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. We also leverage existing frameworks such as Apache Camel K and Kafka (AMQ Streams) to reduce the amount of code that we need to write.&lt;/p&gt; &lt;h3&gt;What is Quarkus?&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://quarkus.io/"&gt;Quarkus&lt;/a&gt; is a Kubernetes-native Java framework crafted from best-of-breed Java libraries and standards. We also sometimes refer to Quarkus as &lt;i&gt;supersonic, subatomic Java&lt;/i&gt;, and for a good reason: Quarkus offers fast boot times and low RSS memory (not just heap size) in container-orchestration platforms like &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Quarkus lets developers create Java applications that have a similar footprint to &lt;a href="https://developers.redhat.com/blog/category/node-js/"&gt;Node.js&lt;/a&gt;, or smaller.&lt;/p&gt; &lt;p&gt;For this demonstration, we chose to run our Quarkus apps on OCP. Running on OpenShift Container Platform means that our demo applications can run anywhere that OpenShift runs, which includes bare metal, Amazon Web Services (AWS), Azure, Google Cloud, IBM Cloud, vSphere, and more.&lt;/p&gt; &lt;h3&gt;What is Red Hat OpenShift?&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.openshift.com/products/container-platform"&gt;Red Hat OpenShift&lt;/a&gt; offers a consistent hybrid-cloud foundation for building and scaling containerized applications. OpenShift provides an enterprise-grade, container-based platform with no vendor lock-in. Red Hat was one of the first companies to work with Google on Kubernetes, even prior to launch, and has become the second leading contributor to the Kubernetes upstream project. Using OpenShift simplifies application deployment because we can easily create resources (such as the MongoDB database we&amp;#8217;re using for this demonstration) by entering just a couple of commands in the terminal. OpenShift also provides a common development platform no matter what infrastructure we use to host the application.&lt;/p&gt; &lt;h3&gt;What is Red Hat AMQ Streams?&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-amq-streams"&gt;AMQ Streams&lt;/a&gt; is an enterprise-grade Apache Kafka (&lt;a href="https://developers.redhat.com/blog/category/stream-processing/"&gt;event streaming&lt;/a&gt;) solution, which enables systems to exchange data at high throughput and low latency. Using queues is a great way to ensure that our applications are loosely coupled. Kafka is an excellent product, providing a highly scalable, fault-tolerant message queue that is capable of handling large volumes of data with relative ease.&lt;/p&gt; &lt;h3&gt;What is Apache Camel K?&lt;/h3&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://camel.apache.org/camel-k/latest/"&gt;Apache Camel K&lt;/a&gt; is a lightweight cloud-integration platform that runs natively on Kubernetes and supports automated cloud configurations. Based on the famous Apache Camel, Camel K is designed and optimized for serverless and microservices architectures. Camel offers hundreds of connectors, providing connectivity to many existing applications, frameworks, and platforms.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;For this demonstration, you will need the following technologies set up in your development environment:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;An OpenShift 4.3+ environment with Cluster Admin access&lt;/li&gt; &lt;li&gt;JDK 11 installed with &lt;code&gt;JAVA_HOME&lt;/code&gt; appropriately configured&lt;/li&gt; &lt;li&gt;Openshift CLI (&lt;code&gt;oc&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Apache Maven 3.6.2+&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We will build two separate Quarkus applications and deploy them to our OpenShift environment. The first application retrieves all of the data from an online repository (the &lt;a target="_blank" rel="nofollow" href="https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports"&gt;Johns Hopkins University GitHub repository&lt;/a&gt;) and uses that data to populate a MongoDB collection called &lt;code&gt;covid19report&lt;/code&gt;. The second application hosts the Quarkusian COVID-19 Tracker website, which dynamically generates charts based on the country that was selected. This application uses REST calls to query the MongoDB collection and returns the relevant data.&lt;/p&gt; &lt;h2&gt;Adding resources to the OpenShift environment&lt;/h2&gt; &lt;p&gt;Before we can get started with the two applications, we need to add the required resources to an OpenShift cluster. We&amp;#8217;ll add a MongoDB database first; then, we will add the Kafka cluster and create the Kafka topic to publish to.&lt;/p&gt; &lt;p&gt;Using &lt;code&gt;oc&lt;/code&gt;, log into your OpenShift environment, and create a new project called &lt;code&gt;covid-19-tracker&lt;/code&gt;. Then, add the MongoDB database to that namespace:&lt;/p&gt; &lt;pre&gt;$ oc new project covid-19-tracker $ oc new-app -n covid-19-tracker --docker-image mongo:4.0 --name=covid19report &lt;/pre&gt; &lt;p&gt;Next, log into the OpenShift console, go to the OperatorHub, and search for the AMQ Streams Operator. Figure 1 shows all of the AMQ installations available from the OperatorHub.&lt;/p&gt; &lt;div id="attachment_730147" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ.png"&gt;&lt;img aria-describedby="caption-attachment-730147" class="wp-image-730147 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ-1024x518.png" alt="A screenshot of the AMQ Streams page in the OperatorHub." width="640" height="324" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ-1024x518.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ-768x389.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/OperatorHub-AMQ.png 1579w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730147" class="wp-caption-text"&gt;Figure 1. Search for the AMQ Streams Operator in the OpenShift OperatorHub.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Install the &lt;b&gt;Red Hat Integration &amp;#8211; AMQ Streams&lt;/b&gt; Operator. After the Operator is successfully installed, go to &lt;b&gt;Installed Operators&lt;/b&gt;, and click on it. You should see a screen similar to Figure 2.&lt;/p&gt; &lt;div id="attachment_730167" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/RedHatIntegration-AMQStreams.png"&gt;&lt;img aria-describedby="caption-attachment-730167" class="wp-image-730167 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/RedHatIntegration-AMQStreams-1024x512.png" alt="A screenshot of all of the available APIs for the Red Hat Integration - AMQ Streams Operator." width="640" height="320" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/RedHatIntegration-AMQStreams-1024x512.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/RedHatIntegration-AMQStreams-300x150.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/RedHatIntegration-AMQStreams-768x384.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730167" class="wp-caption-text"&gt;Figure 2. A listing of available APIs for the Red Hat Integration &amp;#8211; AMQ Streams Operator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Select the Kafka tile and click &lt;b&gt;Create Instance&lt;/b&gt;. Create the Kafka instance with default settings. Creating this instance launches seven pods: One pod is for the Kafka Cluster Entity Operator, and there are three pods each for the Kafka cluster and Zookeeper cluster.&lt;/p&gt; &lt;p&gt;Once all seven pods are running, go back to the &lt;b&gt;Installed Operators &lt;/b&gt;page, and again select the &lt;strong&gt;Red Hat Integration &amp;#8211; AMQ Streams Operator&lt;/strong&gt;. This time, select the &lt;strong&gt;Kafka Topic&lt;/strong&gt; tile and click &lt;b&gt;Create Instance&lt;/b&gt;. You will see the option to create and configure the Kafka topic for our demonstration, as shown in Figure 3.&lt;/p&gt; &lt;div id="attachment_730177" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/CreateKafkaTopic.png"&gt;&lt;img aria-describedby="caption-attachment-730177" class="wp-image-730177" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/CreateKafkaTopic.png" alt="A screenshot of the YAML file to create the Kafka topic." width="640" height="441" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/CreateKafkaTopic.png 720w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/CreateKafkaTopic-300x207.png 300w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730177" class="wp-caption-text"&gt;Figure 3. Create a Kafka topic by manually entering the required YAML or JSON definitions.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;In this case, we need to make just one change to the YAML file. Change the topic&amp;#8217;s name (&lt;code&gt;metadata: name&lt;/code&gt;) to: &lt;code&gt;jhucsse&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Leave everything else in the file as the default values, then create the topic. Now the Kafka environment is ready to accept the data from our Quarkus applications.&lt;/p&gt; &lt;p&gt;For our Quarkus apps to connect to Kafka and MongoDB, we need to make a note of the cluster IP addresses for those services. Run the following from the command line, and you will be presented with a list of services and their corresponding internal IPs:&lt;/p&gt; &lt;pre&gt;$ oc get services &lt;/pre&gt; &lt;p&gt;Figure 4 shows the list of available services and each one&amp;#8217;s internal IP address:&lt;/p&gt; &lt;div id="attachment_730267" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/oc_get_services-3.png"&gt;&lt;img aria-describedby="caption-attachment-730267" class="wp-image-730267" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/oc_get_services-3.png" alt="A screenshot of available services and their associated IP addresses in the terminal." width="640" height="61" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/oc_get_services-3.png 975w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/oc_get_services-3-300x29.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/oc_get_services-3-768x73.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730267" class="wp-caption-text"&gt;Figure 4. Available services in the cluster.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Make a note of the IP address for &lt;code&gt;my-cluster-kafka-bootstrap&lt;/code&gt; and &lt;code&gt;covid19report&lt;/code&gt;. Later, we&amp;#8217;ll add these values to the &lt;code&gt;application.properties&lt;/code&gt; file for each of our Quarkus applications.&lt;/p&gt; &lt;h2&gt;Preparing the Quarkus applications&lt;/h2&gt; &lt;p&gt;Before going any further, you should either download and unzip or clone the two demo applications to your local machine. The source code is available at the following URLs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Application 1 (&lt;code&gt;covid-data-fetching)&lt;/code&gt;&lt;/strong&gt;: &lt;a target="_blank" rel="nofollow" href="https://github.com/gmccarth/covid-data-fetching"&gt;https://github.com/gmccarth/covid-data-fetching&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Application 2 (&lt;code&gt;covid-19-tracker)&lt;/code&gt;&lt;/strong&gt;: &lt;a target="_blank" rel="nofollow" href="https://github.com/gmccarth/covid-19-tracker"&gt;https://github.com/gmccarth/covid-19-tracker&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After you extract the code, you will need to modify the &lt;code&gt;application.properties&lt;/code&gt; file for each application to ensure that the Quarkus apps can connect to the MongoDB database and Kafka cluster we set up earlier.&lt;/p&gt; &lt;h3&gt;Modify the application.properties for Application 1&lt;/h3&gt; &lt;p&gt;In the &lt;code&gt;application.properties&lt;/code&gt; for Application 1 (&lt;code&gt;covid-data-fetching&lt;/code&gt;), find the two lines that start with &lt;code&gt;quarkus.mongodb&lt;/code&gt;. Replace the IP addresses after &lt;code&gt;mongodb://&lt;/code&gt; with the IP address for our MongoDB pod (&lt;code&gt;covid19report&lt;/code&gt;), which you noted earlier. Be sure to include the correct port, which is 27017:&lt;/p&gt; &lt;pre&gt;quarkus.mongodb.connection-string=mongodb://&lt;em&gt;&amp;#60;the IP for covid19report&amp;#62;&lt;/em&gt;:27017 quarkus.mongodb.hosts=mongodb://&lt;em&gt;&amp;#60;the IP for covid19report&amp;#62;&lt;/em&gt;:27017 &lt;em&gt;For example:&lt;/em&gt; quarkus.mongodb.connection-string=mongodb://172.30.195.119:27017 quarkus.mongodb.hosts=mongodb://172.30.195.119:27017&lt;/pre&gt; &lt;p&gt;Similarly, find the &lt;code&gt;camel.component.kafka.brokers&lt;/code&gt; line and replace the IP address with the &lt;code&gt;my-cluster-kafka-bootstrap&lt;/code&gt; IP address. Use port 9092 for this service:&lt;/p&gt; &lt;pre&gt;camel.component.kafka.brokers=&lt;em&gt;&amp;#60;the IP for my-cluster-kafka-bootstrap&amp;#62;&lt;/em&gt;:9092 &lt;/pre&gt; &lt;h3&gt;Modify the application.properties for Application 2&lt;/h3&gt; &lt;p&gt;Now open Application 2 (&lt;code&gt;covid-19-tracker&lt;/code&gt;) and find the &lt;code&gt;quarkus.mongodb.connection-string&lt;/code&gt;. Replace the IP address with the IP address for our MongoDB pod:&lt;/p&gt; &lt;pre&gt;quarkus.mongodb.connection-string=mongodb://&lt;em&gt;&amp;#60;the IP for covid19report&amp;#62;&lt;/em&gt;:27017 &lt;/pre&gt; &lt;p&gt;Similarly, find the &lt;code&gt;camel.component.kafka.brokers&lt;/code&gt; line and replace the IP address with the &lt;code&gt;my-cluster-kafka-bootstrap&lt;/code&gt;IP address. Use port 9092 for this service.&lt;/p&gt; &lt;h2&gt;Set up and run the first application&lt;/h2&gt; &lt;p&gt;For our first application, we use Apache Camel to retrieve files directly from the Johns Hopkins University GitHub repository URL. Camel transforms the CSV files into individual records, which we place into a Kafka topic. A second Camel route then consumes the messages from the Kafka topic. It transforms each record into a database object and inserts that data into a MongoDB collection. We&amp;#8217;ll go through each of these phases in detail.&lt;/p&gt; &lt;h3&gt;Phase 1: Retrieve the data from the repository, transform it, and publish it to a Kafka topic&lt;/h3&gt; &lt;p&gt;Figure 5 shows a flow diagram of the CSV files being retrieved from the GitHub repository and placed in a Kafka topic.&lt;/p&gt; &lt;div id="attachment_730317" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1.png"&gt;&lt;img aria-describedby="caption-attachment-730317" class="wp-image-730317 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1-1024x981.png" alt="A flow diagram of the CSV files being retrieved from the shared repository and placed in the Kafka topic." width="640" height="613" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1-1024x981.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1-300x287.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1-768x735.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase1.png 1130w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730317" class="wp-caption-text"&gt;Figure 5. Retrieve the data from the repository, transform it, and publish it to a Kafka topic.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You can find the code for this phase in the &lt;code&gt;JhuCsseExtractor.java&lt;/code&gt; file:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, we use a Camel route to get the CSV files from the Johns Hopkins University source: &lt;pre&gt;from("timer:jhucsse?repeatCount=1") .setHeader("nextFile", simple("02-01-2020")) .setHeader("version", simple("v1")) .loopDoWhile(method(this, "dateInValidRange(${header.nextFile})")) .setHeader("nextFile", method(this,      "computeNextFile(${header.nextFile})")) .setHeader("version", method(this, "getVersion(${header.nextFile})")) .toD("https:{{jhu.csse.baseUrl}}/${header.nextFile}.csv?httpMethod=GET") .log(LoggingLevel.DEBUG,"after setHeader:nextFile=${header.nextFile}") .split().tokenize("\n", 1, true) .log(LoggingLevel.DEBUG,"version=${header.version}") .choice() .when(header("version").isEqualTo("v1")) .unmarshal().bindy(BindyType.Csv, JhuCsseDailyReportCsvRecordv1.class) .marshal().json(JsonLibrary.Jackson) .to("kafka:jhucsse") .otherwise() .unmarshal().bindy(BindyType.Csv, JhuCsseDailyReportCsvRecordv2.class) .marshal().json(JsonLibrary.Jackson) .to("kafka:jhucsse") .end(); &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Next, we use a &lt;code&gt;loopDoWhile&lt;/code&gt; to fetch all of the CSV files for the specified date range.&lt;/li&gt; &lt;li&gt;At this point, the CSV format changes to include additional data from &lt;code&gt;03-22-2020.csv&lt;/code&gt; onward. We use a &lt;code&gt;choice&lt;/code&gt; method to handle the change in data format. The &lt;code&gt;choice&lt;/code&gt; method ensures that all of the data is correctly inserted into the Kafka topic.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Phase 2: Consume the messages in the Kafka topic and write them to the MongoDB collection&lt;/h3&gt; &lt;p&gt;Figure 6 shows a flow diagram of the transformed records being placed in the MongoDB collection.&lt;/p&gt; &lt;div id="attachment_730337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2.png"&gt;&lt;img aria-describedby="caption-attachment-730337" class="wp-image-730337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2-1024x915.png" alt="A flow diagram of the database objects being sent to the MongoDB database." width="640" height="572" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2-1024x915.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2-300x268.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2-768x686.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Phase2.png 1110w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730337" class="wp-caption-text"&gt;Figure 6. Consume the messages in the Kafka topic and write them to the MongoDB collection.&lt;/p&gt;&lt;/div&gt; &lt;ol&gt; &lt;li&gt;In a different bean (&lt;code&gt;MongoDbPopulator.java&lt;/code&gt;), we configure another Camel route to consume the messages from the Kafka topic we developed in Phase 1. The Camel route will write those messages to our MongoDB database: &lt;pre&gt;fromF("kafka:jhucsse?brokers=%s",brokers)         .log("message: ${body}")         .toF("mongodb:mongoClient?database=%s&amp;#38;collection=%s&amp;#38;operation=insert", database, collection); &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;To run the Quarkus app, in our terminal, we need to be in the &lt;code&gt;../covid-data-fetching/&lt;/code&gt; directory. Type the following into the terminal to kick-off building and deploying the Quarkus application: &lt;pre&gt;./mvnw clean package -Dquarkus.kubernetes.deploy=true -DskipTests=true -Dquarkus.kubernetes-client.trust-certs=true -Dquarkus.s2i.base-jvm-image=fabric8/s2i-java:latest-java11 &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;When you see the &lt;code&gt;BUILD SUCCESS&lt;/code&gt; message, go to your OpenShift console, where the &lt;code&gt;covid-data-fetching&lt;/code&gt; application should be starting to run. To view the Camel route in action, go to the &lt;code&gt;covid19report&lt;/code&gt; pod&amp;#8217;s &lt;strong&gt;Logs&lt;/strong&gt; tab, where you should see something similar to Figure 7, a screenshot of messages flowing into Kafka. &lt;p&gt;&lt;div id="attachment_730187" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs.png"&gt;&lt;img aria-describedby="caption-attachment-730187" class="wp-image-730187 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs-1024x486.png" alt="A screenshot of messages streaming into Kafka." width="640" height="304" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs-1024x486.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs-768x364.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/KafkaStreamingLogs.png 1586w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730187" class="wp-caption-text"&gt;Figure 7. The application logs show a message stream flowing into Kafka.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;li&gt;To confirm that the records are being written to MongoDB, go to the &lt;b&gt;Terminal&lt;/b&gt; tab of the &lt;code&gt;covid19report&lt;/code&gt; pod and type &lt;code&gt;Mongo&lt;/code&gt; in the terminal window. This command launches the MongoDB shell. In the shell type &lt;code&gt;show dbs&lt;/code&gt; to see a list of databases, which should include &lt;code&gt;covid19report&lt;/code&gt;. Figure 8 shows the list of databases. &lt;p&gt;&lt;div id="attachment_730367" style="width: 248px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/MongoShowDbs.png"&gt;&lt;img aria-describedby="caption-attachment-730367" class="wp-image-730367 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/MongoShowDbs.png" alt="A screenshot showing a list of databases, including covid19report." width="238" height="126" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730367" class="wp-caption-text"&gt;Figure 8. The list of databases should include covid19report.&lt;/p&gt;&lt;/div&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Set up and run the second application&lt;/h2&gt; &lt;p&gt;Application 2 is the Quarkusian COVID-19 Tracker web application. It uses REST calls to the MongoDB database to dynamically retrieve a requested data set, then launches the website. Figure 9 shows a flow diagram for the COVID-19 Tracker.&lt;/p&gt; &lt;div id="attachment_730377" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture.png"&gt;&lt;img aria-describedby="caption-attachment-730377" class="wp-image-730377 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture-1024x1015.png" alt="A flow diagram of a COVID-19 data set being retrieved from MongoDB." width="640" height="634" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture-1024x1015.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture-150x150.png 150w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture-300x297.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture-768x761.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Application2Architecture.png 1378w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730377" class="wp-caption-text"&gt;Figure 9. A COVID-19 data set is dynamically retrieved from the MongoDB database.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Notes about this application:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;When we run Quarkus, a web server is started and is accessible on port 8080.&lt;/li&gt; &lt;li&gt;Our Quarkus project included RESTEasy JAX-RS. This allows us to create multiple REST endpoints for the MongoDB queries.&lt;/li&gt; &lt;li&gt;Quarkus also supports dependency injection, so we can easily inject a companion bean into our main class.&lt;/li&gt; &lt;li&gt;Our &lt;code&gt;index.html&lt;/code&gt; page (in the &lt;code&gt;resources/META-INF&lt;/code&gt; folder) has a dropdown list to select the specific, country-based data set that we want to use. The dropdown list is populated from a query to the MongoDB database. On submit, the page sends the country code in a &lt;code&gt;GET&lt;/code&gt; request to the &lt;code&gt;TrackerResource&lt;/code&gt; bean. The bean uses &lt;a target="_blank" rel="nofollow" href="https://quarkus.io/guides/mongodb-panache"&gt;Panache&lt;/a&gt; to query the MongoDB database. It then returns the response to the web page, which generates a graph from the received JSON response.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To run the Quarkus app, we need to be in the &lt;code&gt;../covid-19-tracker/&lt;/code&gt; directory. Type the following into the terminal to kick-off building and deploying this Quarkus application:&lt;/p&gt; &lt;pre&gt;$ ./mvnw clean package -Dquarkus.kubernetes.deploy=true -DskipTests=true -Dquarkus.kubernetes-client.trust-certs=true -Dquarkus.s2i.base-jvm-image=fabric8/s2i-java:latest-java11 &lt;/pre&gt; &lt;p&gt;After you see the &lt;code&gt;BUILD SUCCESS&lt;/code&gt; message, go to your OpenShift console and confirm that the &lt;code&gt;covid-19-tracker&lt;/code&gt; application is starting to run. Once the pod is running, you need to expose the service, so that you can get a route to it from the internet. In the terminal, type:&lt;/p&gt; &lt;pre&gt;$ oc expose svc/covid-19-tracker&lt;/pre&gt; &lt;p&gt;In your OpenShift console, in the administrator&amp;#8217;s perspective, go to &lt;b&gt;Networking&lt;/b&gt; -&amp;#62; &lt;b&gt;Routes&lt;/b&gt; to get the application URL. Click on the URL, which takes you to the application. Try selecting data sets from different countries. You should see something like the screenshot in Figure 10, with the charts changing to show the COVID-19 data for the country that you have selected.&lt;/p&gt; &lt;div id="attachment_730387" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website.png"&gt;&lt;img aria-describedby="caption-attachment-730387" class="wp-image-730387 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website-1024x555.png" alt="A screenshot of the web page showing confirmed cases for the United Kingdom." width="640" height="347" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website-1024x555.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website-768x417.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/Website.png 1366w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-730387" class="wp-caption-text"&gt;Figure 10. The COVID-19 Tracker shows data results from whatever country is selected.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how to extract meaningful visualizations from an external repository with relative ease, and with very few lines of code. We used existing, solid frameworks to reduce complexity and the time required to build a reusable application. Deploying the application to OpenShift reduced the time necessary to develop, build, and deploy the demo applications. Additionally, Quarkus requires substantially less memory than a standard Java application. As a result, we built applications with faster launch times and quicker responses, resulting in an improved experience for developers, end-users, and ultimately, the business.&lt;/p&gt; &lt;p&gt;Are you interested in trying out Quarkus? Check out our self-paced &lt;a href="https://developers.redhat.com/courses/quarkus/getting-started/"&gt;Getting Started with Quarkus&lt;/a&gt; lab!  See the &lt;a href="https://developers.redhat.com/summit/2020/self-paced/"&gt;entire catalog&lt;/a&gt; for more developer labs.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: I would like to thank Mary Cochran, Claus Ibsen, and Josh Reagan, who assisted with troubleshooting and pointed me in the right direction for this article. Special thanks, also, to my fellow Hackfest team members: Jochen Cordes and Bruno Machado, who helped with building the Camel routes and configuring the MongoDB database.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#38;linkname=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Ftracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift%2F&amp;#038;title=Tracking%20COVID-19%20using%20Quarkus%2C%20AMQ%20Streams%2C%20and%20Camel%20K%20on%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2020/06/15/tracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift/" data-a2a-title="Tracking COVID-19 using Quarkus, AMQ Streams, and Camel K on OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/15/tracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift/"&gt;Tracking COVID-19 using Quarkus, AMQ Streams, and Camel K on OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QkUhNGsv0ds" height="1" width="1" alt=""/&gt;</content><summary>In just a matter of weeks, the world that we knew changed forever. The COVID-19 pandemic came swiftly and caused massive disruption to our healthcare systems and local businesses, throwing the world’s economies into chaos. The coronavirus quickly became a crisis that affected everyone. As researchers and scientists rushed to make sense of it, and find ways to eliminate or slow the rate of infectio...</summary><dc:creator>gmccarth</dc:creator><dc:date>2020-06-15T07:00:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/15/tracking-covid-19-using-quarkus-amq-streams-and-camel-k-on-openshift/</feedburner:origLink></entry><entry><title>Supersonic, Subatomic Java Hackathon: June 15 – July 22 2020</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/V95W1arB1GY/" /><category term="cloud native java" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Hackathon" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="kubernetes-native" scheme="searchisko:content:tags" /><category term="quarkus" scheme="searchisko:content:tags" /><category term="serverless" scheme="searchisko:content:tags" /><author><name>jebeck</name></author><id>searchisko:content:id:jbossorg_blog-supersonic_subatomic_java_hackathon_june_15_july_22_2020</id><updated>2020-06-15T07:00:00Z</updated><published>2020-06-15T07:00:00Z</published><content type="html">&lt;p&gt;&lt;span style="font-weight: 400;"&gt;The &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="http://quarkus.io/"&gt;&lt;span style="font-weight: 400;"&gt;Quarkus&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; community is excited to announce the &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://quarkus.devpost.com/"&gt;&lt;span style="font-weight: 400;"&gt;Supersonic, Subatomic Java Hackathon&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; for developers &lt;/span&gt;&lt;span style="font-weight: 400;"&gt;to create Kubernetes-native applications for a chance to win &lt;/span&gt;&lt;b&gt;$30,000&lt;/b&gt;&lt;span style="font-weight: 400;"&gt; in prizes. This hackathon is a great opportunity to learn about the future of &lt;a href="https://developers.redhat.com/blog/2020/04/08/why-kubernetes-native-instead-of-cloud-native/"&gt;cloud-native&lt;/a&gt; Java development and showcase your coding skills.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;If you are new to Quarkus, don’t worry.  The community will be there to help and support you with a number of enablement sessions (see below) throughout the hackathon including an opening ceremony, weekly office hours, and the &lt;/span&gt;&lt;a href="https://developers.redhat.com/devnation/master-course/quarkus/?sc_cid=7013a000002gWC9AAM"&gt;&lt;span style="font-weight: 400;"&gt;DevNation Quarkus Master Course&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; series.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;The hackathon will run from Monday, June 15th through Wednesday, July 22nd culminating in a &amp;#8220;live&amp;#8221; judging and award ceremony on Friday, August 14th.&lt;/span&gt;&lt;/p&gt; &lt;blockquote&gt;&lt;p&gt;To register, &lt;a target="_blank" rel="nofollow" href="https://quarkus.devpost.com/"&gt;click here&lt;/a&gt;!&lt;/p&gt;&lt;/blockquote&gt; &lt;p&gt;&lt;span id="more-732287"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Prize categories&lt;/h2&gt; &lt;p&gt;Five categories of prizes are available:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best Overall: $10,000&lt;/li&gt; &lt;li&gt;Best Serverless App: $5,000&lt;/li&gt; &lt;li&gt;Best Kubernetes-native App: $5,000&lt;/li&gt; &lt;li&gt;Best IoT App: $5,000&lt;/li&gt; &lt;li&gt;Best Migrated App: $5,000&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Judges&lt;/h2&gt; &lt;p&gt;Our panel of judges:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://twitter.com/AdamBien?s=20"&gt;&lt;span style="font-weight: 400;"&gt;Adam Bien&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; (Java Champion, author, consultant)&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://twitter.com/jtgreene?s=20"&gt;&lt;span style="font-weight: 400;"&gt;Jason Greene&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; (Red hat Distinguished Engineer, Quarkus Community Leader) &lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://twitter.com/karesti?s=20"&gt;&lt;span style="font-weight: 400;"&gt;Katia Aresti&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; (Java Champion, Red Hat Senior Software Engineer)&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://www.linkedin.com/in/ken-johnson-kzj/"&gt;&lt;span style="font-weight: 400;"&gt;Ken Johnson&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; (Vice President, Red Hat Application Services)&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;&lt;b&gt;Enablement sessions&lt;/b&gt;&lt;/h2&gt; &lt;p&gt;Attend our enablement sessions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jun 17th @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Opening Ceremony&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Mon, Jun 22nd) DevNation Quarkus &lt;/span&gt;&lt;a href="https://developers.redhat.com/devnation/master-course/quarkus/?sc_cid=7013a000002gWC9AAM"&gt;&lt;span style="font-weight: 400;"&gt;Master Course I&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; &amp;#8211; Quarkus Basics&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jun 24th) DevNation Quarkus &lt;/span&gt;&lt;a href="https://developers.redhat.com/devnation/master-course/quarkus/?sc_cid=7013a000002gWC9AAM"&gt;&lt;span style="font-weight: 400;"&gt;Master Course II&lt;/span&gt;&lt;/a&gt;&lt;span style="font-weight: 400;"&gt; &amp;#8211; Quarkus Cloud-Native&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Thurs, Jun 25th @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Quarkus Office Hours&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jul 1st @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Quarkus Office Hours&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jul 8th @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Quarkus Office Hours&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jul 15th @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Quarkus Office Hours&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Wed, Jul 22nd @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Closing Ceremony&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;(Fri, Aug 14th @ 10 AM EST) &lt;/span&gt;&lt;a target="_blank" rel="nofollow" href="https://youtube.com/quarkusio/live"&gt;&lt;span style="font-weight: 400;"&gt;Live Judging and Winner Announcement&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#38;linkname=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F15%2Fsupersonic-subatomic-java-hackathon-june-15-july-22-2020%2F&amp;#038;title=Supersonic%2C%20Subatomic%20Java%20Hackathon%3A%20June%2015%20%E2%80%93%20July%2022%202020" data-a2a-url="https://developers.redhat.com/blog/2020/06/15/supersonic-subatomic-java-hackathon-june-15-july-22-2020/" data-a2a-title="Supersonic, Subatomic Java Hackathon: June 15 – July 22 2020"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/15/supersonic-subatomic-java-hackathon-june-15-july-22-2020/"&gt;Supersonic, Subatomic Java Hackathon: June 15 &amp;#8211; July 22 2020&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/V95W1arB1GY" height="1" width="1" alt=""/&gt;</content><summary>The Quarkus community is excited to announce the Supersonic, Subatomic Java Hackathon for developers to create Kubernetes-native applications for a chance to win $30,000 in prizes. This hackathon is a great opportunity to learn about the future of cloud-native Java development and showcase your coding skills. If you are new to Quarkus, don’t worry.  The community will be there to help and support ...</summary><dc:creator>jebeck</dc:creator><dc:date>2020-06-15T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/15/supersonic-subatomic-java-hackathon-june-15-july-22-2020/</feedburner:origLink></entry><entry><title>Event streaming and data federation: A citizen integrator’s story</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3vF8QZhP7kA/" /><category term="data federation" scheme="searchisko:content:tags" /><category term="data virtualization" scheme="searchisko:content:tags" /><category term="event-driven" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="Kafka connector" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="Stream Processing" scheme="searchisko:content:tags" /><author><name>snandaku</name></author><id>searchisko:content:id:jbossorg_blog-event_streaming_and_data_federation_a_citizen_integrator_s_story</id><updated>2020-06-12T07:00:33Z</updated><published>2020-06-12T07:00:33Z</published><content type="html">&lt;p&gt;&lt;span style="font-weight: 400;"&gt;Businesses are seeking to benefit from every customer interaction with real-time personalized experience&lt;/span&gt;. Targeting each customer with relevant offers can greatly improve customer loyalty, but we must first understand the customer. We have to be able to draw on data and other resources from diverse systems, such as marketing, customer service, fraud, and business operations. With the advent of modern technologies and agile methodologies, we also want to be able to &lt;a target="_blank" rel="nofollow" href="https://www.computerweekly.com/microscope/opinion/In-pursuit-of-agility-empowering-the-citizen-integrator"&gt;empower citizen integrators&lt;/a&gt; (typically business users who understand business and client needs) to create custom software. What we need is &lt;span style="font-weight: 400;"&gt;one single functional domain where the information is harmonized in a homogeneous way. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;In this article, I will show you how to use &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; to create a personalized customer experience. Figure 1 shows a high-level overview of the integration architecture we&amp;#8217;ll use for the example.&lt;/p&gt; &lt;div id="attachment_731697" style="width: 530px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-731697" class="wp-image-731697" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/high_level_arch-300x179.jpg" alt="" width="520" height="310" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/high_level_arch-300x179.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/high_level_arch.jpg 739w" sizes="(max-width: 520px) 100vw, 520px" /&gt;&lt;p id="caption-attachment-731697" class="wp-caption-text"&gt;Figure 1: Loyalty Management Application&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Let&amp;#8217;s start by looking at the use case for a Loyalty Management application; then, I&amp;#8217;ll introduce the technologies we&amp;#8217;ll use for the integration.&lt;/p&gt; &lt;h2&gt;Loyalty Management Use Case&lt;/h2&gt; &lt;p&gt;Our example application fulfills a use case of sending offers to customers in realtime. When a customer performs a transaction, it enters the event stream. For every event, we fetch the customer context and perform two simple checks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is the customer a &lt;code&gt;PLATINUM&lt;/code&gt; or &lt;code&gt;GOLD&lt;/code&gt; user?&lt;/li&gt; &lt;li&gt;Is the customer&amp;#8217;s predictive loyalty segmentation &lt;code&gt;HIGH&lt;/code&gt; or &lt;code&gt;MEDIUM&lt;/code&gt;?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We&amp;#8217;ll then use this information to determine whether to extend an offer to the given customer.&lt;/p&gt; &lt;h2&gt;Architectural overview&lt;/h2&gt; &lt;p&gt;&lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; is a set of integration and messaging products that provide API connectivity, data transformation, service composition, and more. For the example application, we&amp;#8217;ll use Red Hata Data Virtualization, Red Hat Fuse Online, and Red Hat AMQ Streams. Figure 2 shows the architecture and technologies used by our architecture.&lt;/p&gt; &lt;div id="attachment_731717" style="width: 558px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-731717" class="wp-image-731717" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/tech_stack-300x179.jpg" alt="" width="548" height="327" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/tech_stack-300x179.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/tech_stack.jpg 739w" sizes="(max-width: 548px) 100vw, 548px" /&gt;&lt;p id="caption-attachment-731717" class="wp-caption-text"&gt;Figure 2. Architectural Overview&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We will use Red Hat Integration for both its event streaming infrastructure and its ability to fulfill the required integration capabilities. The architecture&amp;#8217;s backbone is &lt;a href="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/"&gt;Red Hat AMQ Streams&lt;/a&gt;, a massively scalable, distributed, and high-performance data-streaming platform that is based on Apache Kafka.&lt;/p&gt; &lt;p&gt;We&amp;#8217;ll also use the developer preview of &lt;a href="https://developers.redhat.com/blog/2020/01/21/first-steps-with-the-data-virtualization-operator-for-red-hat-openshift/"&gt;Red Hat Data Virtualization&lt;/a&gt;, a container-native service that provides integrated access to diverse data sources. We&amp;#8217;ll use Data Virtualization to collect data from different data sources. We&amp;#8217;ll use the data to create the customer context.&lt;/p&gt; &lt;p&gt;Finally, we&amp;#8217;ll use &lt;a href="https://developers.redhat.com/products/fuse/getting-started"&gt;Red Hat Fuse Online&lt;/a&gt; to create both the integration and the data services. Fuse Online is an Integration Platform-as-a-Service (iPaaS) that makes it easy for business users to collaborate with integration experts and application developers. With Fuse Online&amp;#8217;s low-code tooling, integration experts can quickly create data services and integrations.&lt;/p&gt; &lt;h3&gt;The Integration flow&lt;/h3&gt; &lt;p&gt;Figure 3 shows the sequence of steps required for the example integration.&lt;/p&gt; &lt;div id="attachment_721377" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/integ_map.jpeg"&gt;&lt;img aria-describedby="caption-attachment-721377" class="wp-image-721377" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/integ_map-300x36.jpeg" alt="A linear diagram with icons depicting the integration stack." width="640" height="78" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/integ_map-300x36.jpeg 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/integ_map-768x93.jpeg 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/integ_map-1024x124.jpeg 1024w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721377" class="wp-caption-text"&gt;Figure 3. A preview of the complete integration stack.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The integration begins when the system reads an event from a Transaction topic. The system looks up the customer&amp;#8217;s context and Customer Segmentation. The system then applies checks to ensure that only users who meet the required filter criteria will receive an offer. Finally, if the customer is eligible, the system publishes an offer in a secondary Kafka topic.&lt;/p&gt; &lt;h2&gt;Preparing the environment&lt;/h2&gt; &lt;p&gt;Now that you have an overview of the example integration let&amp;#8217;s set up our demonstration environment. We&amp;#8217;ll use &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; to install the components that we need. OpenShift enables efficient container orchestration, allowing developers to rapidly provision, deploy, scale, and manage container-based applications. We&amp;#8217;ll use Red Hat Integration on OpenShift to rapidly and easily create and manage a web-scale cloud-native integration.&lt;/p&gt; &lt;h3&gt;Step 1: Deploy AMQ Streams&lt;/h3&gt; &lt;p&gt;To start, we&amp;#8217;ll install AMQ Streams from the OpenShift OperatorHub. Begin by logging in to the OpenShift console and creating a new project. Figure 4 shows the selection to install the AMQ Streams Operator from the OperatorHub.&lt;/p&gt; &lt;div id="attachment_732527" style="width: 687px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-732527" class="wp-image-732527" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator-300x152.png" alt="" width="677" height="343" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator-768x390.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator-1024x520.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator.png 1600w" sizes="(max-width: 677px) 100vw, 677px" /&gt;&lt;p id="caption-attachment-732527" class="wp-caption-text"&gt;Figure 4. AMQ Streams Operator&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, create a Kafka cluster with the default settings provided by the AMQ Streams Operator, as shown in Figure 5.&lt;/p&gt; &lt;div id="attachment_732537" style="width: 660px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-732537" class="wp-image-732537" src="https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator_2-300x152.png" alt="" width="650" height="329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator_2-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator_2-768x388.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator_2-1024x517.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/06/fuse_online_operator_2.png 1600w" sizes="(max-width: 650px) 100vw, 650px" /&gt;&lt;p id="caption-attachment-732537" class="wp-caption-text"&gt;Figure 5. Kafka Configuration&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You should now have an ephemeral Kafka cluster.&lt;/p&gt; &lt;h3&gt;Step 2: Deploy Fuse Online&lt;/h3&gt; &lt;p&gt;Now we&amp;#8217;ll use the OperatorHub to deploy Fuse Online, as shown in Figure 6.&lt;/p&gt; &lt;div id="attachment_721407" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3.png"&gt;&lt;img aria-describedby="caption-attachment-721407" class="wp-image-721407" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3-300x150.png" alt="A screenshot of Fuse Online in the OpenShift Operator Hub." width="640" height="320" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3-300x150.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3-768x384.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3-1024x513.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_3.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721407" class="wp-caption-text"&gt;Figure 6. Fuse Online Operator&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 3: Prepare the data&lt;/h3&gt; &lt;p&gt;The final step before we begin the demo is to create the database tables that will hold our application&amp;#8217;s customer and transaction data. We will need to set up a mock event emitter that would mimic the behavior of real customer events flowing through the system. We will also need to set up a mock endpoint for the prediction service, which we&amp;#8217;ll consume from our integration service. This part of the process is a little more involved, so please follow the &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/citizen-integrator-story-assets/blob/master/prepare_data.adoc"&gt;steps described here&lt;/a&gt; to complete the setup.&lt;/p&gt; &lt;h2&gt;Low-code tooling with Fuse Online&lt;/h2&gt; &lt;p&gt;Once the infrastructure is set up, we can log in to the Fuse Online console. You can find the URL for the console under the routes. You should be able to log in with your OpenShift console credentials.&lt;/p&gt; &lt;h3&gt;Step 1: Create connections&lt;/h3&gt; &lt;p&gt;First, we&amp;#8217;ll need to set up two connections: one for the PostgreSQL database and the other for MySQL. From Fuse Online&amp;#8217;s &lt;strong&gt;Connections&lt;/strong&gt; tab, click on &lt;strong&gt;Create Connections&lt;/strong&gt; and choose &lt;strong&gt;Database&lt;/strong&gt;, as shown in Figure 7. Setup the connection credentials for the Postgres DB.&lt;/p&gt; &lt;div id="attachment_721417" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4.png"&gt;&lt;img aria-describedby="caption-attachment-721417" class="wp-image-721417" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4-300x152.png" alt="A screenshot of the dialog to create a database connection." width="640" height="324" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4-768x388.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4-1024x518.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_4.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721417" class="wp-caption-text"&gt;Figure 7. Create a database connection for PostgreSQL.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Follow the same process to create the connection string for the MySQL database, using the connection parameters shown in Figure 8.&lt;/p&gt; &lt;div id="attachment_721427" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5.png"&gt;&lt;img aria-describedby="caption-attachment-721427" class="wp-image-721427" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5-300x148.png" alt="A screenshot of the dialog and parameters for the MySQL connection." width="640" height="316" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5-300x148.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5-1024x506.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_5.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721427" class="wp-caption-text"&gt;Figure 8. Create a database connection for MySQL.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;The Kafka connector&lt;/h4&gt; &lt;p&gt;Next, we&amp;#8217;ll create a Kafka connector so that we can read and publish events to a customer events stream. Choose the &lt;strong&gt;Kafka Message Broker&lt;/strong&gt; connection type under &lt;strong&gt;Create Connections&lt;/strong&gt;, then add the URL of the previously created Kafka cluster, as shown in Figure 9.&lt;/p&gt; &lt;div id="attachment_721437" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6.png"&gt;&lt;img aria-describedby="caption-attachment-721437" class="wp-image-721437" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6-300x140.png" alt="A screenshot of the dialog to add the Kafka cluster URL." width="640" height="299" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6-300x140.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6-768x359.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6-1024x479.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_6.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721437" class="wp-caption-text"&gt;Figure 9. Add the URL of the Kafka cluster.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;The API client connection&lt;/h4&gt; &lt;p&gt;Finally, we&amp;#8217;ll set up an API client connection to mock the prediction API. &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/citizen-integrator-story-assets/blob/master/api_json.json"&gt;Upload this JSON file&lt;/a&gt; to create the prediction-service connector, as shown in Figure 10.&lt;/p&gt; &lt;div id="attachment_721447" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7.png"&gt;&lt;img aria-describedby="caption-attachment-721447" class="wp-image-721447" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7-300x156.png" alt="A screenshot of the dialog to upload the JSON file." width="640" height="332" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7-300x156.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7-768x399.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7-1024x532.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_7.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721447" class="wp-caption-text"&gt;Figure 10. Upload the JSON file in the API client connector wizard.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Follow the directions to save the API client connector when you are done.&lt;/p&gt; &lt;p&gt;Now we&amp;#8217;re ready to begin creating the integration. We&amp;#8217;ll start with the data service.&lt;/p&gt; &lt;h3&gt;Step 2: Create the data service&lt;/h3&gt; &lt;p&gt;Start by selecting the &lt;strong&gt;Data&lt;/strong&gt; option from Fuse Online&amp;#8217;s left-hand pane, then click on &lt;strong&gt;Create Data Virtualization&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;Create a view&lt;/h4&gt; &lt;p&gt;All of our connections appear in the view editor.  Select the &lt;strong&gt;Transaction&lt;/strong&gt; and &lt;strong&gt;Customer&lt;/strong&gt; tables, as shown in Figure 11.&lt;/p&gt; &lt;div id="attachment_721467" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8.png"&gt;&lt;img aria-describedby="caption-attachment-721467" class="wp-image-721467" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8-300x138.png" alt="A screenshot of the dialog to select the required tables." width="640" height="295" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8-300x138.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8-768x354.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8-1024x472.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_8.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721467" class="wp-caption-text"&gt;Figure 11. Select the Transaction and Customer tables.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can also use the view editor to create a &lt;strong&gt;Virtual Database&lt;/strong&gt; table with the consolidated customer context, as shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_721487" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9.png"&gt;&lt;img aria-describedby="caption-attachment-721487" class="wp-image-721487" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9-300x152.png" alt="A screenshot of the dialog to create a virtual database table." width="640" height="324" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9-768x389.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9-1024x518.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_online_operator_9.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721487" class="wp-caption-text"&gt;Figure 12. Create a Virtual Database table to host the customer context.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The view editor also provides a data section where we will be able to test the results of a query.&lt;/p&gt; &lt;h4&gt;Publish and access the data service&lt;/h4&gt; &lt;p&gt;Now we are ready to publish the data service. Once it&amp;#8217;s published, we can use either the &lt;a target="_blank" rel="nofollow" href="https://docs.oracle.com/javase/tutorial/jdbc/basics/index.html"&gt;Java Database Connectivity (JDBC) API&lt;/a&gt; or an &lt;a target="_blank" rel="nofollow" href="https://www.odata.org"&gt;OData endpoint&lt;/a&gt; to access the virtual database. Select the OData endpoint, as shown in Figure 13.&lt;/p&gt; &lt;div id="attachment_721497" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1.png"&gt;&lt;img aria-describedby="caption-attachment-721497" class="wp-image-721497" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1-300x71.png" alt="A screenshot of the dialog to select the OData endpoint." width="640" height="152" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1-300x71.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1-768x182.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1-1024x243.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_1.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721497" class="wp-caption-text"&gt;Figure 13. Select the OData endpoint to access the virtual database.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;As shown in Figure 14, we can use the URL format &lt;strong&gt;&amp;#60;odata-link&amp;#62;/odata/&amp;#60;virtualization-name&amp;#62;/&amp;#60;view-name&amp;#62; &lt;/strong&gt;to access the OData endpoint.&lt;/p&gt; &lt;div id="attachment_721507" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2.png"&gt;&lt;img aria-describedby="caption-attachment-721507" class="wp-image-721507" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2-300x97.png" alt="A screenshot of the URL format to access the virtual database." width="640" height="206" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2-300x97.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2-768x248.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2-1024x330.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_2.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721507" class="wp-caption-text"&gt;Figure 14. Access the virtual database via its OData endpoint.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 3: Create the integration service&lt;/h3&gt; &lt;p&gt;Now we&amp;#8217;ll create an integration service.&lt;/p&gt; &lt;h4&gt;Subscribe and Publish to Kafka&lt;/h4&gt; &lt;p&gt;In the Fuse Online console, go to the &lt;strong&gt;Integration&lt;/strong&gt; tab and click &lt;strong&gt;Create Integration&lt;/strong&gt;. As shown in Figure 15, we can read from the Kafka topic where customer events are created.&lt;/p&gt; &lt;div id="attachment_721517" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3.png"&gt;&lt;img aria-describedby="caption-attachment-721517" class="wp-image-721517" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3-300x107.png" alt="A screenshot of the dialog to configure a customer event." width="640" height="227" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3-300x107.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3-768x273.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3-1024x364.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_3.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721517" class="wp-caption-text"&gt;Figure 15. Configure the Kafka Subscribe Step.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We&amp;#8217;ll use a JSON instance to define the message&amp;#8217;s data structure:&lt;/p&gt; &lt;pre&gt;{"eventValue": "MERCHANDISE", "eventSource": "POS","custId":"CUST898920"} &lt;/pre&gt; &lt;p&gt;As shown in Figure 16, we&amp;#8217;ll define the integration&amp;#8217;s last step, where we&amp;#8217;ll write the offer results back to a Kafka topic. We will define the JSON instance for the customer&amp;#8217;s offer data, as follows:&lt;/p&gt; &lt;pre&gt;{"offer": "value", "custId":"id","customerClass":"class","customersegmentation":"segment"} &lt;/pre&gt; &lt;div id="attachment_721527" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4.png"&gt;&lt;img aria-describedby="caption-attachment-721527" class="wp-image-721527" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4-300x133.png" alt="A screenshot showing the option to configure the endpoint for a Kafka event." width="640" height="283" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4-300x133.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4-768x339.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4-1024x452.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_4.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721527" class="wp-caption-text"&gt;Figure 16. Configure the Kafka Publish Step.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;Fetch the customer data&lt;/h4&gt; &lt;p&gt;Next, we fetch our customer data from the virtual database. In the Fuse Online console, select the &lt;strong&gt;Virtual DB&lt;/strong&gt; connection that we configured earlier, then choose the &lt;strong&gt;Invoke SQL&lt;/strong&gt; option.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll define intermediate steps before reaching that endpoint. For every customer record, we will fetch the customer&amp;#8217;s virtual data context, which we created earlier. Figure 17 shows the dialog to fetch the virtual data context.&lt;/p&gt; &lt;div id="attachment_721537" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6.png"&gt;&lt;img aria-describedby="caption-attachment-721537" class="wp-image-721537" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6-300x136.png" alt="A screenshot of the dialog to create the Kafka event and the option to fetch the virtual data context." width="640" height="290" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6-300x136.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6-768x348.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6-1024x463.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_6.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721537" class="wp-caption-text"&gt;Figure 17. Fetch the virtual data context.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Figure 18 is a screenshot of how the integration looks so far.&lt;/p&gt; &lt;div id="attachment_721547" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7.png"&gt;&lt;img aria-describedby="caption-attachment-721547" class="wp-image-721547" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7-300x117.png" alt="The dialog shows the integration with steps added so far and the option to add more steps." width="640" height="250" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7-300x117.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7-768x300.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7-1024x401.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_7.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721547" class="wp-caption-text"&gt;Figure 18. A screenshot of the integration in progress.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;Fetch the prediction data&lt;/h4&gt; &lt;p&gt;Next, we&amp;#8217;ll add a step to fetch the prediction data from our mock prediction service. Click on the plus (+) symbol after Step 2 and choose the prediction-service connection that we created earlier. This step is shown in Figure 19.&lt;/p&gt; &lt;div id="attachment_721557" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8.png"&gt;&lt;img aria-describedby="caption-attachment-721557" class="wp-image-721557" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8-300x85.png" alt="A screenshot of the dialog to add the prediction text." width="640" height="180" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8-300x85.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8-768x216.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8-1024x289.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_8.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721557" class="wp-caption-text"&gt;Figure 19. Fetch the prediction data.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;Define a data mapper for each connection&lt;/h4&gt; &lt;p&gt;After each of the connection steps, you will now see a warning to define the data mapper. The data mapper maps a customer&amp;#8217;s input values to the correct output values.  To start defining the data mappers, click on the plus (+) symbol immediately after Step 1. As shown in Figure 20, this will give you the option to define the target step.&lt;/p&gt; &lt;div id="attachment_721577" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9.png"&gt;&lt;img aria-describedby="caption-attachment-721577" class="wp-image-721577" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9-300x121.png" alt="A screenshot of the dialog to map to the correct database." width="640" height="258" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9-300x121.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9-768x310.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9-1024x413.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_9.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721577" class="wp-caption-text"&gt;Figure 20. Mapping the Source and Target steps.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Follow the same process to define the mapper that will be called when we invoke the prediction service, and then the mapper that will be called before we publish to the Kafka topic.&lt;/p&gt; &lt;p&gt;As shown in Figure 21, we use the data-mapper dialog to map the values from each of the steps so that we can select the correct offer for the customer. Also, note that we are using this dialog to add a transformation to the offer text.&lt;/p&gt; &lt;div id="attachment_721587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10.png"&gt;&lt;img aria-describedby="caption-attachment-721587" class="wp-image-721587" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10-300x146.png" alt="A screenshot of the dialog to map the values of each step." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10-300x146.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10-768x373.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10-1024x498.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_10.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721587" class="wp-caption-text"&gt;Figure 21. Mapper before publish.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 4: Apply the filter criteria&lt;/h3&gt; &lt;p&gt;We&amp;#8217;ve used Fuse Online to connect all the pieces of our integration. However, you might have noticed that we are missing an important piece. Before we can publish our integration, we need to ensure that we will only extend an offer to customers who have a PLATINUM or GOLD status, and whose predictive loyalty segmentation is labeled either HIGH and MEDIUM. For this, we will configure the &lt;strong&gt;Basic Filter&lt;/strong&gt; option shown in Figure 22.&lt;/p&gt; &lt;div id="attachment_721597" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11.png"&gt;&lt;img aria-describedby="caption-attachment-721597" class="wp-image-721597" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11-300x152.png" alt="A screenshot of the dialog selecting the basic-filter option." width="640" height="323" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11-300x152.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11-768x388.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11-1024x517.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_11.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721597" class="wp-caption-text"&gt;Figure 22. Configure the Basic Filter option for customer status and predictive loyalty segmentation.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Step 5: Publish and monitor the integration&lt;/h3&gt; &lt;p&gt;Now that we are done assembling all of the pieces, it is time to publish. Just save the integration and publish it. After it is published, the integration stack should look like what you see in Figure 23.&lt;/p&gt; &lt;div id="attachment_721607" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12.png"&gt;&lt;img aria-describedby="caption-attachment-721607" class="wp-image-721607" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12-300x77.png" alt="" width="640" height="163" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12-300x77.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12-768x196.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12-1024x261.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_12.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721607" class="wp-caption-text"&gt;Figure 23. The complete stack for the Loyalty Management Application.&lt;/p&gt;&lt;/div&gt; &lt;h4&gt;Monitor the integration&lt;/h4&gt; &lt;p&gt;We can use the &lt;strong&gt;Activity&lt;/strong&gt; tab to view events as they are being processed, as shown in Figure 24.&lt;/p&gt; &lt;div id="attachment_721617" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13.png"&gt;&lt;img aria-describedby="caption-attachment-721617" class="wp-image-721617" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13-300x148.png" alt="A screenshot of the Activity tab." width="640" height="315" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13-300x148.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13-768x378.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13-1024x504.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_13.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721617" class="wp-caption-text"&gt;Figure 24. Use the Activity tab to view events as they are being processed.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;We can use the &lt;strong&gt;Metrics&lt;/strong&gt; tab to view the overall metrics for processed messages, as shown in Figure 25.&lt;/p&gt; &lt;div id="attachment_721627" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14.png"&gt;&lt;img aria-describedby="caption-attachment-721627" class="wp-image-721627" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14-300x80.png" alt="A screenshot of the Metrics tab." width="640" height="171" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14-300x80.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14-768x205.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14-1024x273.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/fuse_onliner_14.png 1600w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-721627" class="wp-caption-text"&gt;Figure 25. Use the Metrics tab to view the overall metrics for received and processed Kafka messages.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Visit my GitHub repository to &lt;a target="_blank" rel="nofollow" href="https://github.com/snandakumar87/citizen-integrator-story-assets/blob/master/portfolio-integration-export.zip"&gt;download the complete integration code&lt;/a&gt;. If you want to import the code to Fuse Online, navigate to the &lt;strong&gt;Integration&lt;/strong&gt; tab, and click &lt;strong&gt;Import&lt;/strong&gt;.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Using the data-first approach, we are now able to quickly visualize the context information required for our near real-time processing requirements. By providing low code tooling we are able to empower the citizen integrators and data experts to quickly create these integration solutions in a cloud-native fashion. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#38;linkname=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fevent-streaming-and-data-federation-a-citizen-integrators-story%2F&amp;#038;title=Event%20streaming%20and%20data%20federation%3A%20A%20citizen%20integrator%E2%80%99s%20story" data-a2a-url="https://developers.redhat.com/blog/2020/06/12/event-streaming-and-data-federation-a-citizen-integrators-story/" data-a2a-title="Event streaming and data federation: A citizen integrator’s story"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/12/event-streaming-and-data-federation-a-citizen-integrators-story/"&gt;Event streaming and data federation: A citizen integrator&amp;#8217;s story&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3vF8QZhP7kA" height="1" width="1" alt=""/&gt;</content><summary>Businesses are seeking to benefit from every customer interaction with real-time personalized experience. Targeting each customer with relevant offers can greatly improve customer loyalty, but we must first understand the customer. We have to be able to draw on data and other resources from diverse systems, such as marketing, customer service, fraud, and business operations. With the advent of mod...</summary><dc:creator>snandaku</dc:creator><dc:date>2020-06-12T07:00:33Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/12/event-streaming-and-data-federation-a-citizen-integrators-story/</feedburner:origLink></entry><entry><title>How to install CodeReady Workspaces in a restricted OpenShift 4 environment</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/H1rPmYaIm6g/" /><category term="air gapped" scheme="searchisko:content:tags" /><category term="Developer Tools" scheme="searchisko:content:tags" /><category term="Eclipse Che" scheme="searchisko:content:tags" /><category term="Enterprise Java" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="operator" scheme="searchisko:content:tags" /><category term="pull image" scheme="searchisko:content:tags" /><category term="Universal Base Images (UBI)" scheme="searchisko:content:tags" /><author><name>Bryant Son</name></author><id>searchisko:content:id:jbossorg_blog-how_to_install_codeready_workspaces_in_a_restricted_openshift_4_environment</id><updated>2020-06-12T07:00:04Z</updated><published>2020-06-12T07:00:04Z</published><content type="html">&lt;p&gt;It&amp;#8217;s your first day as a &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; programmer, right out of college. You have received your badge, a shiny new laptop, and all of your software requests have been approved. Everything seems to be going well.&lt;/p&gt; &lt;p&gt;You install Eclipse and set up the required Java Development Kit (JDK) in your new development environment. You clone a project from the company&amp;#8217;s GitHub repository, modify the code, and make your first commit. You are excited to be working on your first project.&lt;/p&gt; &lt;p&gt;But then, a few hours later, a senior programmer asks what version of the JDK you used. It seems that the pipeline is reporting a project failure. All you did was commit Java source code, not binary, and it worked perfectly on your local machine. What could possibly have gone wrong?&lt;/p&gt; &lt;h2&gt;Coding in a restricted environment&lt;/h2&gt; &lt;p&gt;The issue I described is well-known among programmers as the &amp;#8220;&lt;a target="_blank" rel="nofollow" href="https://imgs.xkcd.com/comics/inexplicable.png"&gt;It works on my computer, and I don&amp;#8217;t know why it doesn&amp;#8217;t work on your computer&lt;/a&gt;&amp;#8221; problem. Fortunately, this is the type of problem &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces (CRW)&lt;/a&gt; can help you solve. CodeReady Workspaces is a cloud-based IDE based on &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/che"&gt;Che&lt;/a&gt;. Whereas Che is an open source project, CRW is an enterprise-ready development environment that provides the security, stability, and consistency that many corporations require. All you have to do is open the CRW link in a web browser, sign in with your user credentials, and code inside the browser.&lt;/p&gt; &lt;p&gt;In this article, I show you how to install CodeReady Workspaces in a restricted &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift 4&lt;/a&gt; environment.&lt;/p&gt; &lt;p&gt;&lt;span id="more-657467"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Example setup and prerequisites&lt;/h2&gt; &lt;p&gt;Ideally, you should be able to pull images directly from the &lt;a target="_blank" rel="nofollow" href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Registry&lt;/a&gt; and use them for your installation. After all, images in the Red Hat Registry are secure and certified. In some cases, however, you will find that the company’s network is behind a firewall, or that the network policy does not allow you to pull images directly from the Red Hat Registry. For this article, I assume that you can neither pull images nor enable a proxy to pull images directly from the Red Hat Registry. Instead, I show you how to pull the required images and stash them in the company&amp;#8217;s private registry, such as Artifactory or Nexus. You can then use the privately stored images to install CodeReady Workspaces.&lt;/p&gt; &lt;p&gt;I assume the following about your environment and the installation process:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The cloud platform is OpenShift 4&lt;/strong&gt;: You should have a running OpenShift 4 environment, and you should know how to use the OpenShift user interface (UI) or command-line interface (CLI).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;We&amp;#8217;re installing CodeReady Workspaces version 2.0 or higher&lt;/strong&gt;: Most of this tutorial is applicable to CRW 1.2, but I assume that you are using at least CodeReady Workspaces 2.0.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;We will use OpenShift 4&amp;#8217;s OperatorHub for installation&lt;/strong&gt;: You could use another method to install CodeReady Workspaces, but I will demonstrate how to use OpenShift 4&amp;#8217;s built-in OperatorHub.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The environment is a restricted network&lt;/strong&gt;: Although you could set up the example in a non-restricted environment, I assume that you are working in a restricted network.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You have access to the Red Hat Registry&lt;/strong&gt;: You should be able to log in and retrieve container images from the Red Hat Registry.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You have access to a private registry&lt;/strong&gt;: After you pull container images from the Red Hat Registry, you need to store them in a private registry, such as Artifactory or Nexus.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note that my example is also based on an &lt;i&gt;air-gapped installation&lt;/i&gt;, which is a disconnected installation that supports working in a restricted environment.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s get started.&lt;/p&gt; &lt;h2&gt;Step 1: Pull the required CRW images from the Red Hat Registry&lt;/h2&gt; &lt;p&gt;Installing CRW 2.0 in a restricted environment requires pulling 13 images from the &lt;a target="_blank" rel="nofollow" href="https://catalog.redhat.com/software/containers/explore"&gt;Red Hat Container Registry&lt;/a&gt;, which is shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_656707" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-656707" class="wp-image-656707 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-19-at-8.58.58-PM-1024x497.png" alt="A screenshot of installation images for CodeReady Workshop in the Red Hat Container Registry." width="640" height="311" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-19-at-8.58.58-PM-1024x497.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-19-at-8.58.58-PM-300x146.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-19-at-8.58.58-PM-768x373.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-656707" class="wp-caption-text"&gt;Figure 1. Find the required images for your CodeReady Workshop installation in the Red Hat Container Registry.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The first five images have been required since CodeReady Workspaces 1.2, and the eight new images are required to deploy CRW 2.0:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;codeready-workspaces/server-operator-rhel8:2.0&lt;/code&gt;: The CodeReady Workspaces Operator orchestrates and manages the installation process. It is especially important to have this Operator installed for OpenShift 4.x.&lt;/li&gt; &lt;li&gt;&lt;code&gt;codeready-workspaces/server-rhel8:2.0&lt;/code&gt;: This is the Che server. It provides the main platform to manage workspaces and aspects of the project, such as programming stacks, user groups, and the factory. You will also use the Che server to view your project dashboard.&lt;/li&gt; &lt;li&gt;&lt;code&gt;redhat-sso-7/sso73-openshift:latest&lt;/code&gt;: CRW uses Red Hat Single Sign-On (SSO), which is based on &lt;a target="_blank" rel="nofollow" href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt;. This enterprise implementation is compliant with SAML 2.0 and OpenID. You will need it to authenticate, authorize, and manage users in CodeReady Workspaces.&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Factory is JavaScript Object Notation (JSON) file that defines the elements, including where to find the source code for the workspace, what languages are used in the project, the commands to pre-populate in the IDE for the workspaces, as well as any post-load actions to be performed on the workspace automatically after it is built.&lt;/p&gt; &lt;ol start="4"&gt; &lt;li&gt;&lt;code&gt;rhscl/postgresql-96-rhel7:latest&lt;/code&gt;: Keycloak writes to the PostgreSQL database. You will need this image to store user-related data.&lt;/li&gt; &lt;li&gt;&lt;code&gt;ubi8-minimal:latest&lt;/code&gt;: The universal base image (UBI) provides the persistent volume that you will use to store data for your workspace and anything else that is required for CRW.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You also need to import the following images:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/pluginregistry-rhel8:2.0&lt;/code&gt;: Makes it possible to share a plug-in definition across all the users of the same instance of CodeReady Workspaces. Only plug-ins that are published in a registry can be used in a devfile.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/devfileregistry-rhel8:2.0&lt;/code&gt;: Holds the definitions of the CodeReady Workspaces stacks. These are available on the CodeReady Workspaces user dashboard when selecting &lt;strong&gt;Create Workspace&lt;/strong&gt;. It contains the list of CodeReady Workspaces technological stack samples with example projects.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/pluginbroker-rhel8:2.0&lt;/code&gt;: Ensures via this Operator that all installed plug-ins are handled correctly.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/pluginbrokerinit:2.0&lt;/code&gt;: Runs as an init container to ensure that all installed plugins are handled correctly.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/jwtproxy-rhel8:2.0&lt;/code&gt;: Implements the self-signed per-workspace JWT token and its verification on a dedicated service based on JWT proxy.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/machineexec-rhel8:2.0&lt;/code&gt;: Runs Go-lang server-side creation for machine-execs for CRW workspaces.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/theia-rhel8:2.0&lt;/code&gt;: Defines the default web IDE for the workspace based on &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse-theia/theia"&gt;the Theia project&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;registry.redhat.io/codeready-workspaces/theia-endpoint-rhel8:2.0&lt;/code&gt;: Similar to the theia-rhel8 image above, this adds the Theia components for CRW&amp;#8217;s IDE look and feel.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From CRW 2.1, there seems to be significant updates to the required images listed above. For example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;codeready-workspaces/server-operator-rhel8:2.0&lt;/code&gt; is gone and replaced with &lt;code&gt;codeready-workspaces/crw-2-rhel8-operator:2.1&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;pluginbroker-rhel8&lt;/code&gt; and &lt;code&gt;pluginbrokerinit:2.0&lt;/code&gt; are replaced with &lt;code&gt;codeready-workspaces/pluginbroker-artifacts-rhel8:2.1&lt;/code&gt; and &lt;code&gt;codeready-workspaces/pluginbroker-metadata-rhel8:2.1&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;theia-dev-rhel8:2.1&lt;/code&gt; was added to the list of images.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you end up using CRW 2.1, please make the appropriate changes.&lt;/p&gt; &lt;h3&gt;Download the images&lt;/h3&gt; &lt;p&gt;When you locate a container image that you need, select the image, and click the &lt;strong&gt;Tags&lt;/strong&gt; tab to see the image name and version information. You will specify this information later, to pull the images from the Red Hat Container Registry. As of this writing, the version number for the CodeReady Workspaces Operator is 2.0, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_660557" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-660557" class="wp-image-660557 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screen-Shot-2019-12-02-at-4.30.03-PM-1024x626.png" alt="A screenshot showing the CodeReady Workspaces Operator image in the Red Hat Container Registry." width="640" height="391" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screen-Shot-2019-12-02-at-4.30.03-PM-1024x626.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screen-Shot-2019-12-02-at-4.30.03-PM-300x183.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/Screen-Shot-2019-12-02-at-4.30.03-PM-768x470.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-660557" class="wp-caption-text"&gt;Figure 2. Locate the CodeReady Workspaces Operator in the Red Hat Container Registry.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Unless you have access to a common service account or another authentication mechanism, you will need to create an account before you can download images from the registry. Open the &lt;strong&gt;Get This Image&lt;/strong&gt; tab shown in Figure 3 and follow the detailed instructions.&lt;/p&gt; &lt;div id="attachment_657507" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657507" class="wp-image-657507 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-20-at-10.52.50-PM-1024x627.png" alt="A screenshot of the dialog to create a Red Hat Registry service account." width="640" height="392" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-20-at-10.52.50-PM-1024x627.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-20-at-10.52.50-PM-300x184.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-11-20-at-10.52.50-PM-768x471.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657507" class="wp-caption-text"&gt;Figure 3. Create a service account for the Red Hat Registry.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Each private registry has its own approach to pulling images and storing them. For Artifactory, you could use something like the following &lt;code&gt;curl&lt;/code&gt; command, which you would update with your own variable values:&lt;/p&gt; &lt;pre&gt;$ curl -u JFROG_USERNAME:JFROG_PASSWORD ARTIFACTORY_URL/IMAGE_NAME/metadata/IMAGE_VERSION -k &lt;/pre&gt; &lt;p&gt;Follow the instructions specified by your registry, as well as any requirements that are particular to your organization.&lt;/p&gt; &lt;h3&gt;Verify the images&lt;/h3&gt; &lt;p&gt;Once you pull the images from the Red Hat Registry, verify that they are stored in your private registry. Figure 4 is a screenshot of my Artifactory registry, where I stored the images required for my CodeReady Workspaces installation.&lt;/p&gt; &lt;div id="attachment_657027" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657027" class="wp-image-657027 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/artifactory_screen-1024x667.png" alt="A screenshot of the Artifactory console showing the images that are required to install CodeReady Workspaces." width="640" height="417" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/artifactory_screen-1024x667.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/artifactory_screen-300x196.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/artifactory_screen-768x500.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657027" class="wp-caption-text"&gt;Figure 4. Find the CodeReady Workspaces images stored in Artifactory.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The images that I have outlined in red are required to install CodeReady Workspaces. Use the latest versions available from the Red Hat Registry. The images outlined in blue are &lt;em&gt;stacks&lt;/em&gt;, which represent different programming runtimes. Stacks aren&amp;#8217;t required for your CRW installation, but you will use them in conjunction with a &lt;code&gt;devfile&lt;/code&gt; to create new projects. I will introduce you to the process of importing stacks in Part 2. The image outlined in green (in the upper-right corner of the screenshot) is a package name.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Currently, only OpenShift 4 supports the CodeReady Workspaces Operator, which is key to the installation described in this article. While you could theoretically install CodeReady Workspaces in Kubernetes, I assume that you are installing on OpenShift 4.&lt;/p&gt; &lt;h2&gt;Step 2: Create a new OpenShift 4 project&lt;/h2&gt; &lt;p&gt;Creating a new project in OpenShift is the same whether you are using OpenShift 3 or OpenShift 4. Either way, you can use the OpenShift user interface (UI) or the &lt;a target="_blank" rel="nofollow" href="https://docs.openshift.com/container-platform/4.2/cli_reference/openshift_cli/getting-started-cli.html"&gt;OpenShift command-line interface (CLI)&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Creating a new project with the OpenShift UI&lt;/h3&gt; &lt;p&gt;The OpenShift UI is easy to use. Click &lt;strong&gt;Create Project&lt;/strong&gt; and enter your project information in the remaining fields. Note that you only need to specify the project name. OpenShift uses this information to create a namespace. In the project dialog shown in Figure 5, I entered the project name: &lt;code&gt;crw-demo&lt;/code&gt;.&lt;/p&gt; &lt;div id="attachment_657447" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657447" class="wp-image-657447 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/crw_creation-1-1024x370.jpg" alt="A screenshot showing the dialog to create a new OpenShift project." width="640" height="231" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/crw_creation-1-1024x370.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/crw_creation-1-300x108.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/crw_creation-1-768x277.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/crw_creation-1.jpg 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657447" class="wp-caption-text"&gt;Figure 5. Create a new OpenShift project.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Creating a new project with the OpenShift CLI&lt;/h3&gt; &lt;p&gt;Your other option is to use the OpenShift Container Platform (OCP) CLI. In this case, you would open the command line and enter the command &lt;code&gt;oc new-project&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ oc new-project crw-demo &lt;/pre&gt; &lt;p&gt;Once again, I named my new project &lt;code&gt;crw-demo&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;When you create a new project, OpenShift creates new resources for the project, including two service accounts that you will use later. These are named &lt;strong&gt;default&lt;/strong&gt; and &lt;strong&gt;builder&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Next, we&amp;#8217;ll generate an access token and add it to your OpenShift secrets. You will need this token to pull images from your private registry.&lt;/p&gt; &lt;h2&gt;Step 3: Generate an access token and add it to OpenShift secrets&lt;/h2&gt; &lt;p&gt;The diagram in Figure 6 shows the general process for generating an access token, which you&amp;#8217;ll use to authenticate your user ID and gain access to your private registry. The actual process will vary depending on the application, policy, and environment.&lt;/p&gt; &lt;div id="attachment_657017" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657017" class="wp-image-657017 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/2_diagram_token-1024x561.jpg" alt="A screenshot showing the dialog to create an OpenShift secret." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/2_diagram_token-1024x561.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/2_diagram_token-300x164.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/2_diagram_token-768x421.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/2_diagram_token.jpg 1500w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657017" class="wp-caption-text"&gt;Figure 6. Create an OpenShift secret, which you will use to authenticate your ID in the private registry.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Authorization consists of four steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Request an access token from a service such as &lt;a target="_blank" rel="nofollow" href="https://oauth.net"&gt;OAuth&lt;/a&gt;. The service will generate the token and send it to the private registry. (In some cases you might use a proxy to request the token.)&lt;/li&gt; &lt;li&gt;Call into the private registry and retrieve the token.&lt;/li&gt; &lt;li&gt;Use the token to create an OpenShift secret as a Docker registry.&lt;/li&gt; &lt;li&gt;Add the OpenShift secret containing the token to each of your service accounts in CodeReady Workspaces. (The CodeReady Workspaces Operator requires various service accounts for installation. I&amp;#8217;ll explain this further shortly.)&lt;/li&gt; &lt;/ol&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: In some environments, the process of generating and authenticating the token is done by a developer, whereas in others it is facilitated by a proxy. Either way, it is essential to select a token that does not expire. Another option would be to use a tool like &lt;a target="_blank" rel="nofollow" href="https://www.vaultproject.io"&gt;Vault&lt;/a&gt; to rotate the token and refresh OpenShift&amp;#8217;s secret mechanism. Describing that process is beyond the scope of this article.&lt;/p&gt; &lt;h3&gt;Generating the token manually&lt;/h3&gt; &lt;p&gt;If you are manually generating the access token, you could use a &lt;code&gt;curl&lt;/code&gt; command as simple as this:&lt;/p&gt; &lt;pre&gt;$ curl -u PRIVATE_REGISTRY_USERNAME:PRIVATE_REGISTRY_PASSWORD -X POST PRIVATE_REGISTRY_TOKEN_URL &lt;/pre&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;PRIVATE_REGISTRY_USERNAME&lt;/code&gt; is your user name to log in to the private registry.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PRIVATE_REGISTRY_PASSWORD&lt;/code&gt; is your password to log in to the private registry.&lt;/li&gt; &lt;li&gt;&lt;code&gt;PRIVATE_REGISTRY_URL&lt;/code&gt; is the web address to generate the access token from the private registry.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Again, the mechanism will vary depending on your environment and other factors. Check the documentation for your private registry or seek advice from your team before attempting to generate an access token.&lt;/p&gt; &lt;h3&gt;Creating the OpenShift secret&lt;/h3&gt; &lt;p&gt;Once you generate the token, you can create an OpenShift secret to store the token in a Docker registry. For this, you would use the &lt;code&gt;create secret&lt;/code&gt; command on the OC CLI:&lt;/p&gt; &lt;pre&gt;$ oc create secret docker-registry PULL_SECRET_NAME --docker-server=URL_IMAGE_PRIVATE_REGISTRY \ --docker-username=USERNAME_PRIVATE_REGISTRY \ --docker-password=TOKEN_PRIVATE_REGISTRY \ --docker-email=EMAIL_PRIVATE_REGISTRY &lt;/pre&gt; &lt;p&gt;Where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;PULL_SECRET_NAME&lt;/code&gt; is the OpenShift secret name.&lt;/li&gt; &lt;li&gt;&lt;code&gt;URL_IMAGE_PRIVATE_REGISTRY&lt;/code&gt; is the private registry path containing the images you want to pull.&lt;/li&gt; &lt;li&gt;&lt;code&gt;USERNAME_PRIVATE_REGISTRY&lt;/code&gt; is your user name for accessing the private registry.&lt;/li&gt; &lt;li&gt;&lt;code&gt;TOKEN_PRIVATE_REGISTRY&lt;/code&gt; is the private registry token you have just generated.&lt;/li&gt; &lt;li&gt;&lt;code&gt;EMAIL_PRIVATE_REGISTRY&lt;/code&gt; is the email you have associated with the private registry.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In Figure 7, you can see that my token, &lt;strong&gt;artif-ocp4-sec&lt;/strong&gt;, is stored as an OpenShift secret.&lt;/p&gt; &lt;div id="attachment_657907" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-657907" class="wp-image-657907 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/secret1-1024x586.jpg" alt="A screenshot of the OpenShift Secrets dashboard showing stored tokens." width="640" height="366" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/secret1-1024x586.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/secret1-300x172.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/secret1-768x439.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/secret1.jpg 1288w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-657907" class="wp-caption-text"&gt;Figure 7. Find your private-registry token stored as an OpenShift secret.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;With that, you have now created an Openshift secret with the credentials necessary to pull the image from your private registry. You still need to link the OpenShift secret to your service accounts. Before I show you how to do that, you must install CodeReady Workspaces.&lt;/p&gt; &lt;h2&gt;Step 4: Use the OpenShift OperatorHub to install CodeReady Workspaces&lt;/h2&gt; &lt;p&gt;One way to install CodeReady Workspaces is to leverage &lt;code&gt;&lt;a target="_blank" rel="nofollow" href="https://github.com/che-incubator/chectl"&gt;chectl&lt;/a&gt;&lt;/code&gt;, which is Che&amp;#8217;s command-line interface. Another way, available starting with OpenShift 4, is to use the OpenShift OperatorHub. Figure 8 shows the dialog to locate and install the CodeReady Workspaces Operator from the OperatorHub. (Note that the OperatorHub also contains the Che Operator.)&lt;/p&gt; &lt;div id="attachment_656777" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-656777" class="wp-image-656777 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.00-PM-1024x556.png" alt="A screenshot of CodeReady Workspaces Operator stored in the OpenShift Operator Hub." width="640" height="348" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.00-PM-1024x556.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.00-PM-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.00-PM-768x417.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-656777" class="wp-caption-text"&gt;Figure 8. Find the CodeReady Workspaces Operator in the OpenShift Operator Hub.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you don&amp;#8217;t see the CodeReady Workspaces Operator in your OperatorHub, check the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/operators/olm-adding-operators-to-a-cluster#olm-installing-operators-from-operatorhub_olm-adding-operators-to-a-cluster"&gt;Operator installation instructions&lt;/a&gt; in the Red Hat OpenShift 4.3 documentation.&lt;/p&gt; &lt;h3&gt;Installing CRW from the OperatorHub&lt;/h3&gt; &lt;p&gt;If you go to the OpenShift OperatorHub and select the CodeReady Workspaces Operator, you should see an option to install it. Before pressing the &lt;strong&gt;Install&lt;/strong&gt; button, as shown in Figure 9, make a note of the CRW Operator version (currently 2.0) and the location of the container image. By default, the location points to &lt;strong&gt;registry.redhat.io&lt;/strong&gt;. Later, you will change this location to point to your private registry.&lt;/p&gt; &lt;div id="attachment_656897" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-656897" class="wp-image-656897 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.17-PM-1024x561.png" alt="A screenshot of the option to install the CodeReady Workspaces Operator." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.17-PM-1024x561.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.17-PM-300x164.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.17-PM-768x420.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-656897" class="wp-caption-text"&gt;Figure 9. Install the CodeReady Workspaces Operator from the OpenShift Operator Hub.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After clicking &lt;strong&gt;Install&lt;/strong&gt;, you will be asked to create an Operator subscription. Make sure that you are in the correct namespace. Click the &lt;strong&gt;Subscribe&lt;/strong&gt; button to subscribe to CodeReady Workspaces Operator, as shown in Figure 10.&lt;/p&gt; &lt;div id="attachment_656907" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-656907" class="size-large wp-image-656907" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.36-PM-1024x562.png" alt="A screenshot of the option to create a subscription for the CodeReady Workspaces Operator." width="640" height="351" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.36-PM-1024x562.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.36-PM-300x165.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.36-PM-768x421.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-656907" class="wp-caption-text"&gt;Figure 10. Create a subscription for the CodeReady Workspaces Operator.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The installation can take a while, so be patient. Once the installation is complete, click the &lt;strong&gt;CodeReady Workspaces Operator&lt;/strong&gt; link to open your new CRW instance, which is shown in Figure 11.&lt;/p&gt; &lt;div id="attachment_656927" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-656927" class="size-large wp-image-656927" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.54-PM-1024x556.png" alt="A screenshot of the new CodeReady Workspaces Operator instance stored in our demo project namespace." width="640" height="348" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.54-PM-1024x556.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.54-PM-300x163.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/Screen-Shot-2019-09-17-at-12.25.54-PM-768x417.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-656927" class="wp-caption-text"&gt;Figure 11. Find the CodeReady Workspaces Operator installed in your project namespace.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Next, we&amp;#8217;ll set up a CRW cluster.&lt;/p&gt; &lt;h2&gt;Step 5: Modify CRW&amp;#8217;s custom resource definition file&lt;/h2&gt; &lt;p&gt;So far, you might wonder what about this CRW installation is unique to working in a restricted environment. That will change with this step, where we first create a new CodeReady Workspaces cluster, then modify CRW&amp;#8217;s custom resource definition (CRD) file.&lt;/p&gt; &lt;p&gt;From the CRW project namespace page shown in Figure 11, click the link for your new CodeReady Workspaces Operator. You will be presented with an option to create a new instance of a Red Hat CodeReady Workspaces cluster. Click the &lt;strong&gt;Create Instance&lt;/strong&gt; option, which is shown in Figure 12.&lt;/p&gt; &lt;div id="attachment_658377" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658377" class="size-large wp-image-658377" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster1-1024x687.jpg" alt="A screenshot of the option to create a new CodeReady Workspaces cluster." width="640" height="429" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster1-1024x687.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster1-300x201.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster1-768x515.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster1.jpg 1496w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658377" class="wp-caption-text"&gt;Figure 12. Create a new CodeReady Workspaces Operator cluster.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now, you will be provided with the custom resource definition file for your CodeReady Workspaces Operator. You will need to customize this file, shown in Figure 13, before installing CRW to OpenShift.&lt;/p&gt; &lt;div id="attachment_658387" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658387" class="size-large wp-image-658387" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster2-1024x853.jpg" alt="A screenshot of the custom resource definition file in YAML." width="640" height="533" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster2-1024x853.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster2-300x250.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/create_cluster2-768x640.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658387" class="wp-caption-text"&gt;Figure 13. The custom resource definition file in YAML format.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;The CRD definition is in YAML format, but note that a section of the file is formatted using JSON. You will modify the properties to include the following seven images, which we pulled from the Red Hat Registry in Step 1:&lt;/p&gt; &lt;pre&gt;codeready-workspaces/server-operator-rhel8:2.0 cheImage: codeready-workspaces/server-rhel8:2.0 identityProviderImage: redhat-sso-7/sso73-openshift:latest postgresImage: rhscl/postgresql-96-rhel7:latest pvcJobsImage: ubi8-minimal:latest devfileRegistryImage: codeready-workspaces/devfileregistry-rhel8:2.0 pluginRegistryImage: codeready-workspaces/pluginregistry-rhel8:2.0 &lt;/pre&gt; &lt;p&gt;You can find the full CRD file for the Che Operator on the &lt;a target="_blank" rel="nofollow" href="https://github.com/eclipse/che-operator/blob/master/deploy/crds/org_v1_che_cr.yaml"&gt;Che Operator&amp;#8217;s GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Updating the CRD file&lt;/h3&gt; &lt;p&gt;Note that &lt;code&gt;codeready-workspaces/server-operator-rhel8:2.0&lt;/code&gt; is referenced twice in the CRD file. You will need to add the other images as new properties, as shown in this example:&lt;/p&gt; &lt;pre&gt;"spec": { "server": { ... // Other Properties, # server image used in Che deployment "cheImage": "" }, "database": { ... // Other Properties, "postgresImage": "" }, "storage": { ... // Other Properties, "pvcJobsImage": "" }, "auth": { ... // Other Properties, "identityProviderImage": "" } &lt;/pre&gt; &lt;p&gt;You can also change other properties in this file. For example, I had to change the following two properties to true when I deployed my CRW.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;tlsSupport: true&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;selfSignedCert: true&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Dealing with pod failure&lt;/h3&gt; &lt;p&gt;When you proceed with installing a new CRW instance, your pods will probably fail. To check this in the OpenShift console, go to &lt;strong&gt;Workloads&lt;/strong&gt; -&amp;#62; &lt;strong&gt;Pods&lt;/strong&gt;, then click the pod that you are interested in. As an example, you could click &lt;strong&gt;Log&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you prefer to use the OpenShift CLI, enter &lt;code&gt;oc get pods&lt;/code&gt;, then, &lt;code&gt;oc logs -f po/POD_NAME&lt;/code&gt;. For &lt;code&gt;POD_NAME&lt;/code&gt;, enter the pod you want to verify.&lt;/p&gt; &lt;p&gt;If you inspect the error, you will see that the image-pull event has failed with an error message: &lt;code&gt;ErrImagePull&lt;/code&gt;. In the next section, I&amp;#8217;ll show you how to fix this error by adding your OpenShift secret to the service accounts you will need to run CodeReady Workspaces in a restricted environment.&lt;/p&gt; &lt;h2&gt;Step 6: Add your OpenShift secret to new service accounts&lt;/h2&gt; &lt;p&gt;A CodeReady Workspaces deployment uses the CRW Operator to install and manage all of CRW&amp;#8217;s life-cycle processes. These processes also require leveraging various service accounts. In overview, the flow looks something like this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The &lt;code&gt;codeready-operator&lt;/code&gt; service accounts are initialized and attempt to pull the &lt;code&gt;codeready-workspaces/server-operator-rhel8&lt;/code&gt; image.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;deployer&lt;/code&gt; and &lt;code&gt;builder&lt;/code&gt; service accounts attempt to pull additional required images such as &lt;code&gt;redhat-sso-7/sso73-openshift&lt;/code&gt;, &lt;code&gt;rhscl/postgresql-96-rhel7&lt;/code&gt;, and &lt;code&gt;ubi8-minimal&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;che&lt;/code&gt; service account is created to pull the &lt;code&gt;codeready-workspaces/server-rhel8&lt;/code&gt; image.&lt;/li&gt; &lt;li&gt;Once CRW is up, the &lt;code&gt;che-workspace&lt;/code&gt; service account is used to pull in stack images for various runtimes.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can find the above service accounts in the OpenShift console by clicking &lt;strong&gt;Explore&lt;/strong&gt; and browsing until you find the &lt;strong&gt;ServiceAccounts&lt;/strong&gt; page, as shown in Figure 14.&lt;/p&gt; &lt;div id="attachment_658637" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658637" class="size-large wp-image-658637" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount1-1024x811.jpg" alt="A screenshot of the ServiceAccounts option in the OpenShift console." width="640" height="507" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount1-1024x811.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount1-300x237.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount1-768x608.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount1.jpg 1478w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658637" class="wp-caption-text"&gt;Figure 14. Find the ServiceAccounts page in the OpenShift 4 console.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Alternatively, on the command line, you could enter &lt;code&gt;oc get sa&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Adding more service accounts&lt;/h3&gt; &lt;p&gt;The first time that you try to install CRW, you should only see the &lt;code&gt;codeready-operator&lt;/code&gt; service accounts, which are &lt;strong&gt;default&lt;/strong&gt;, &lt;strong&gt;builder&lt;/strong&gt;, and &lt;strong&gt;deployer&lt;/strong&gt;. You will have to use your OpenShift secret to add the additional service accounts. In the custom resource definition file, create a new line under &lt;code&gt;imagePullSecrets&lt;/code&gt; and add a new OpenShift secret there. Alternatively, you could override the previous OpenShift secret, as shown in Figure 15.&lt;/p&gt; &lt;div id="attachment_658647" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658647" class="size-large wp-image-658647" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount3-1024x804.jpg" alt="A screenshot showing the OpenShift secret added to a new service account in the CRD file." width="640" height="503" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount3-1024x804.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount3-300x235.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount3-768x603.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount3.jpg 1203w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658647" class="wp-caption-text"&gt;Figure 15. Add your OpenShift secret to the service accounts you want to add.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Repeat this step for all of the service accounts you need for your CodeReady Workspaces deployment.&lt;/p&gt; &lt;h3&gt;Updating the deployment script&lt;/h3&gt; &lt;p&gt;Simply modifying your service accounts will not automatically trigger the changes required to deploy a CRW application in a restricted environment. You also need to modify your deployment script, as shown in Figure 16.&lt;/p&gt; &lt;div id="attachment_658657" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658657" class="size-large wp-image-658657" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/success3-1024x891.jpg" alt="A screenshot of the deployment script." width="640" height="557" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/success3-1024x891.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/success3-300x261.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/success3-768x669.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658657" class="wp-caption-text"&gt;Figure 16. Modify your OpenShift deployment script to redeploy the application.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;After you&amp;#8217;ve made these changes, check the pods and their logs to see whether you are able to successfully pull the images that are required for your deployment, as shown in Figure 17. Repeat the steps for any additional service accounts to pull the images that you need.&lt;/p&gt; &lt;div id="attachment_658487" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2.jpg"&gt;&lt;img aria-describedby="caption-attachment-658487" class="wp-image-658487 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2-1024x758.jpg" alt="A screenshot of the CodeReady Workspaces Service Accounts page." width="640" height="474" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2-1024x758.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2-300x222.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2-768x569.jpg 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/serviceaccount2.jpg 1475w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-658487" class="wp-caption-text"&gt;Figure 17. View all of your pod instances in the Service Accounts page.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Step 7: Check your pods and deploy CodeReady Workspaces&lt;/h2&gt; &lt;p&gt;Once everything is up and running, you should see all of your pods are in the &lt;strong&gt;Running&lt;/strong&gt; state, as shown in Figure 18.&lt;/p&gt; &lt;div id="attachment_658677" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658677" class="wp-image-658677 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod1-1-1024x609.jpg" alt="A screenshot of the Running tab open on the Pods page in the OpenShift console." width="640" height="381" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod1-1-1024x609.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod1-1-300x179.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod1-1-768x457.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658677" class="wp-caption-text"&gt;Figure 18. Check the Running tab on the Pods page to find out whether your pods are successfully installed.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;If you notice any issues with a running pod, check for log or event errors, as shown in Figure 19.&lt;/p&gt; &lt;div id="attachment_658687" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658687" class="wp-image-658687 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod2-1024x704.jpg" alt="A screenshot of the OpenShift console showing log data for a running pod." width="640" height="440" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod2-1024x704.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod2-300x206.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/pod2-768x528.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658687" class="wp-caption-text"&gt;Figure 19. Check for logging or event errors in your running pods.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Assuming all of your pods are running smoothly, you can now deploy your CodeReady Workspaces instance in a restricted environment, as shown in Figure 20.&lt;/p&gt; &lt;div id="attachment_658697" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-658697" class="wp-image-658697 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/7-1024x381.jpg" alt="A screenshot showing the Che project page and the link to deploy CodeReady Workspaces in your environment." width="640" height="238" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/7-1024x381.jpg 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/7-300x112.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/7-768x286.jpg 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-658697" class="wp-caption-text"&gt;Figure 20. The route for a successful Che deployment.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;That should be it for installing and deploying CodeReady Workspaces. You might encounter additional challenges related to working in a restricted environment.&lt;/p&gt; &lt;p&gt;For now, you should at least have CodeReady Workspaces installed and ready to use in OpenShift 4. However, that is just a start with your journey to the world of CodeReady Workspaces. In my next articles, I will cover how you can further configure CRW after installation, how to try out various development options in CRW, and how to install the monitoring solutions like Prometheus and Grafana in CRW. Until then, see you next time, and feel free to leave comments.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#38;linkname=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F12%2Fhow-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment%2F&amp;#038;title=How%20to%20install%20CodeReady%20Workspaces%20in%20a%20restricted%20OpenShift%204%20environment" data-a2a-url="https://developers.redhat.com/blog/2020/06/12/how-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment/" data-a2a-title="How to install CodeReady Workspaces in a restricted OpenShift 4 environment"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/12/how-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment/"&gt;How to install CodeReady Workspaces in a restricted OpenShift 4 environment&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/H1rPmYaIm6g" height="1" width="1" alt=""/&gt;</content><summary>It’s your first day as a Java programmer, right out of college. You have received your badge, a shiny new laptop, and all of your software requests have been approved. Everything seems to be going well. You install Eclipse and set up the required Java Development Kit (JDK) in your new development environment. You clone a project from the company’s GitHub repository, modify the code, and make your ...</summary><dc:creator>Bryant Son</dc:creator><dc:date>2020-06-12T07:00:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/12/how-to-install-codeready-workspaces-in-a-restricted-openshift-4-environment/</feedburner:origLink></entry><entry><title>First look at the new Apicurio Registry UI and Operator</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/R2rhs9iezhY/" /><category term="API registry" scheme="searchisko:content:tags" /><category term="Apicurio" scheme="searchisko:content:tags" /><category term="confluent schema registry" scheme="searchisko:content:tags" /><category term="Containers" scheme="searchisko:content:tags" /><category term="devops" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="operator" scheme="searchisko:content:tags" /><category term="schema registry" scheme="searchisko:content:tags" /><author><name>Hugo Guerrero</name></author><id>searchisko:content:id:jbossorg_blog-first_look_at_the_new_apicurio_registry_ui_and_operator</id><updated>2020-06-11T07:00:54Z</updated><published>2020-06-11T07:00:54Z</published><content type="html">&lt;p&gt;Last year, the &lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/"&gt;Apicurio&lt;/a&gt; developer community launched the new &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry"&gt;Apicurio Registry&lt;/a&gt; project, which is an API and schema registry for microservices. You can use the Apicurio Registry to store and retrieve service artifacts such as OpenAPI specifications and AsyncAPI definitions, as well as schemas such as Apache Avro, JSON, and Google Protocol Buffers.&lt;/p&gt; &lt;p&gt;Because the registry also works as a catalog where you can navigate through artifacts, adding a new web-based user interface (UI) was a priority for the current Apicurio Registry 1.2.2 release. With this release, the Apicurio community has made the Apicurio Registry available as a binary download or from container images. To make it easier to set up and manage your Apicurio Registry deployment, they have also created a new &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry-operator"&gt;Kubernetes Operator for the Apicurio Registry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article is a quick introduction to the new Apicurio Registry UI and Apicurio Registry Operator. I&amp;#8217;ll show you how to access these new features in Apicurio 1.2.2 and describe a few highlights of using them. For a more detailed demonstration, check out my video tutorial introducing the new UI and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/kubernetes/operators/"&gt;Operator&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-727447"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/6v4PS5vaiZc?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;The Apicurio Registry UI&lt;/h2&gt; &lt;p&gt;Apicurio&amp;#8217;s new web-based user interface allows you to navigate through artifacts stored in the Registry. You can use the UI to search for artifacts by label, name, or description. You can also preview an artifact&amp;#8217;s content, and you have the option to download and store an artifact locally. While in the registry, you can check for all of the available versions of any stored artifact.&lt;/p&gt; &lt;p&gt;In addition to browsing through artifacts, the Apicurio UI allows you to configure and manage the registry&amp;#8217;s content rules, both globally and for each artifact. Of course, it is also possible to upload new artifacts and update the content or version of any existing artifact.&lt;/p&gt; &lt;p&gt;To access the Apicurio UI, just navigate to the main endpoint of your current Apicurio Registry deployment, such as &lt;code&gt;http://localhost:8080/&lt;/code&gt;. At the endpoint, you will be redirected to the &lt;code&gt;/ui&lt;/code&gt; path.&lt;/p&gt; &lt;h2&gt;The Apicurio Registry Operator&lt;/h2&gt; &lt;p&gt;You can deploy the Apicurio Registry Operator in any working Kubernetes or OpenShift cluster, and use it to quickly install and configure an Apicurio Registry deployment. See the &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry-operator#quickstart"&gt;Apicurio Registry Operator deployment quickstart&lt;/a&gt; for instructions. In the future, the Operator will be available from the Kubernetes &lt;a target="_blank" rel="nofollow" href="https://operatorhub.io"&gt;operatorhub.io&lt;/a&gt; catalog.&lt;/p&gt; &lt;p&gt;After installation, you can create an ApicurioRegistry resource (see the deployment quickstart&amp;#8217;s &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry-operator/blob/master/docs/resources/example-cr/in-memory.yaml"&gt;in-memory example&lt;/a&gt;) and configure it to be deployed in your cluster. If you want to customize the deployed ApicurioRegistry resource, you can define things like the persistence type and the ingress-route configuration to expose it externally.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The Apicurio Registry and Apicurio Registry Operator make it easier to discover, manage, and work with service specifications and artifacts—not only for request-response synchronous APIs but also for asynchronous and event-driven architectures. For a &lt;a href="https://developers.redhat.com/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/"&gt;technical preview and supported version of Apicurio Registry&lt;/a&gt;, check out the &lt;a target="_blank" rel="nofollow" href="https://www.redhat.com/en/products/integration"&gt;Red Hat Integration&lt;/a&gt; service registry. This service registry is a &lt;a href="https://developers.redhat.com/blog/2019/12/17/replacing-confluent-schema-registry-with-red-hat-integration-service-registry/"&gt;drop-in replacement for the Confluent Schema Registry&lt;/a&gt; and is designed to help teams govern their service schemas.&lt;/p&gt; &lt;p&gt;Meanwhile, the Apicurio community continues enhancing Apicurio&amp;#8217;s component ecosystem. The new Apicurio Registry Operator is still in development, but we invite you to try it out and &lt;a target="_blank" rel="nofollow" href="https://github.com/Apicurio/apicurio-registry-operator"&gt;provide feedback to the team&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#38;linkname=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Ffirst-look-at-the-new-apicurio-registry-ui-and-operator%2F&amp;#038;title=First%20look%20at%20the%20new%20Apicurio%20Registry%20UI%20and%20Operator" data-a2a-url="https://developers.redhat.com/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/" data-a2a-title="First look at the new Apicurio Registry UI and Operator"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/"&gt;First look at the new Apicurio Registry UI and Operator&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/R2rhs9iezhY" height="1" width="1" alt=""/&gt;</content><summary>Last year, the Apicurio developer community launched the new Apicurio Registry project, which is an API and schema registry for microservices. You can use the Apicurio Registry to store and retrieve service artifacts such as OpenAPI specifications and AsyncAPI definitions, as well as schemas such as Apache Avro, JSON, and Google Protocol Buffers. Because the registry also works as a catalog where ...</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2020-06-11T07:00:54Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/</feedburner:origLink></entry><entry><title>Extending Red Hat SSO with IBM Security Verify</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/wljMWZV0s_w/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="event-driven" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Identity Management" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="openshift" scheme="searchisko:content:tags" /><category term="passwordless" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="single sign-on" scheme="searchisko:content:tags" /><category term="sso" scheme="searchisko:content:tags" /><author><name>mspatel</name></author><id>searchisko:content:id:jbossorg_blog-extending_red_hat_sso_with_ibm_security_verify</id><updated>2020-06-11T07:00:24Z</updated><published>2020-06-11T07:00:24Z</published><content type="html">&lt;p&gt;More and more organizations are using &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat Single Sign-On (Red Hat SSO)&lt;/a&gt; as the foundation for securing user identities for enterprise and consumer applications. The focus on providing both robust security and a seamless user experience needs to be equally considered. Neither of these requirements should be compromised, especially as applications are being built for a multi-cloud world on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Red Hat SSO plus IBM Security Verify&lt;/h2&gt; &lt;p&gt;With Red Hat SSO plus &lt;a href="https://github.com/IBM-Security/verify-keycloak-integration" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;IBM Security Verify extensions&lt;/a&gt;, developers can continue building applications protected by Red Hat SSO, but also add another layer of advanced authentication and ID-less/passwordless experiences. &lt;a href="https://www.ibm.com/security/identity-access-management/cloud-identity" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;IBM Security Verify&lt;/a&gt; extends the ability for Red Hat SSO developers to create authentication flows that extend QRCode, Mobile Push, FIDO, SMS, and email as different authenticators that can be used as first- or second-factor authentication. In the case of QRCode and FIDO, these can be used for ID-less and passwordless authentication, providing a frictionless end-user experience.&lt;/p&gt; &lt;p&gt;Red Hat SSO developers can download and place the extensions into their Red Hat SSO environment to create the necessary authentication flows. In the case of email and SMS second-factor authentication, Verify maintains and manages all of the necessary components that Red Hat SSO developers would have to configure themselves, such as SMTP and SMPP servers, as part of the Verify identity-as-a-service experience.&lt;/p&gt; &lt;p&gt;Using QRCode as an example, a Red Hat SSO user can easily configure ID-less and passwordless experiences natively within Red Hat SSO to provide a more secure and frictionless authentication flow. All Red Hat SSO developers need to do is create an API client with Verify, drop the Verify extensions in their Red Hat SSO deployment, and configure the QRCode authentication flows within Red Hat SSO (see Figure 1). The steps to add other authentication flows are just a matter of modifying the last step: Configuring the authentication flows.&lt;/p&gt; &lt;div id="attachment_725197" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2.png"&gt;&lt;img aria-describedby="caption-attachment-725197" class="wp-image-725197 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2-1024x387.png" alt="Screenshot of the example authentication flow" width="640" height="242" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2-1024x387.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2-300x113.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2-768x291.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/M2.png 1068w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-725197" class="wp-caption-text"&gt;Figure 1: Example authentication flow with IBM Security Verify QRCode.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;By using Verify, Red Hat SSO developers can also get visibility and analytics into authentication events when different authenticators are being used with applications protected by Red Hat SSO. This provides Red Hat SSO developers the ability to detect where users are authenticating from, determining and seeing any potential anomalies by being able to drill down into each of the events, and more. These types of reports may be used for auditing and compliance purposes, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_725247" style="width: 649px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Picture1-1.png"&gt;&lt;img aria-describedby="caption-attachment-725247" class="wp-image-725247" src="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Picture1-1.png" alt="Screenshot of chart and graph showing authentication event summary and details" width="639" height="314" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/05/Picture1-1.png 928w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Picture1-1-300x147.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/05/Picture1-1-768x377.png 768w" sizes="(max-width: 639px) 100vw, 639px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-725247" class="wp-caption-text"&gt;Figure 2: Viewing authentication event summary and details.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Getting started&lt;/h2&gt; &lt;p&gt;Within twenty minutes, advanced authentication and frictionless experiences can be added to Red Hat SSO deployments:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a &lt;a href="https://www.ibm.com/account/reg/signup?formid=urx-44536" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;free Verify tenant&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the extensions and drop them into a Red Hat SSO deployment.&lt;/li&gt; &lt;li&gt;Configure your desired authentication flows. The user guide can provide more details as needed.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Additional information can be found in the &lt;a href="https://github.com/IBM-Security/verify-keycloak-integration" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;IBM Security GitHub&lt;/a&gt;. You can also get more details for a step-by-step walkthrough of using Verify extensions with Red Hat SSO through the &lt;a href="https://github.com/IBM-Security/verify-keycloak-integration#usage" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;user guides&lt;/a&gt;, along with documentation for a &lt;a href="https://github.com/jason-choi1/ibm-airways-keycloak-sso" target="_blank" target="_blank" rel="nofollow" noreferrer"&gt;sample application&lt;/a&gt; that developers can get started with and explore further.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#38;linkname=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2020%2F06%2F11%2Fextending-red-hat-sso-with-ibm-security-verify%2F&amp;#038;title=Extending%20Red%20Hat%20SSO%20with%20IBM%20Security%20Verify" data-a2a-url="https://developers.redhat.com/blog/2020/06/11/extending-red-hat-sso-with-ibm-security-verify/" data-a2a-title="Extending Red Hat SSO with IBM Security Verify"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2020/06/11/extending-red-hat-sso-with-ibm-security-verify/"&gt;Extending Red Hat SSO with IBM Security Verify&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/wljMWZV0s_w" height="1" width="1" alt=""/&gt;</content><summary>More and more organizations are using Red Hat Single Sign-On (Red Hat SSO) as the foundation for securing user identities for enterprise and consumer applications. The focus on providing both robust security and a seamless user experience needs to be equally considered. Neither of these requirements should be compromised, especially as applications are being built for a multi-cloud world on Red Ha...</summary><dc:creator>mspatel</dc:creator><dc:date>2020-06-11T07:00:24Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2020/06/11/extending-red-hat-sso-with-ibm-security-verify/</feedburner:origLink></entry><entry><title>Apache Camel K 1.0 is here - Why should you care</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9pfCGtn2LMs/apache-camel-k-10-is-here-why-should.html" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="camelk" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_clausibsen" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Claus Ibsen</name></author><id>searchisko:content:id:jbossorg_blog-apache_camel_k_1_0_is_here_why_should_you_care</id><updated>2020-06-10T12:51:42Z</updated><published>2020-06-10T12:51:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Yesterday we released Apache Camel K 1.0 and it was announced on social media and on the Camel website.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-pR00B9yyG-A/XuDPVAWVljI/AAAAAAAACL8/34TAMqn0VHcI_zUi8bhP5-P3cGmxijTdwCLcBGAsYHQ/s1600/Screenshot%2B2020-06-09%2Bat%2B20.46.38.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="426" data-original-width="1169" height="145" src="https://1.bp.blogspot.com/-pR00B9yyG-A/XuDPVAWVljI/AAAAAAAACL8/34TAMqn0VHcI_zUi8bhP5-P3cGmxijTdwCLcBGAsYHQ/s400/Screenshot%2B2020-06-09%2Bat%2B20.46.38.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;So what is Camel K and why should you care? That is a great question and I want to help answer this by referring to great minds.&lt;br /&gt;&lt;br /&gt;Hugo Guerrero posted the &lt;a href="https://twitter.com/hguerreroo/status/1270471245813907463?s=20"&gt;following tweet&lt;/a&gt; &amp;nbsp; &lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://twitter.com/hguerreroo/status/1270471245813907463?s=20"&gt;&lt;img border="0" data-original-height="107" data-original-width="559" height="76" src="https://1.bp.blogspot.com/-_iQRpqOkJBc/XuDQJpydziI/AAAAAAAACMI/_EhBEuh64CMERITk98fi2LOFKjMHFBB4ACLcBGAsYHQ/s400/Screenshot%2B2020-06-10%2Bat%2B14.16.25.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;That is a powerful statement from Hugo, where he highlights the groundbreaking innovation from Camel K, that gives developers the tools and means to build Java based services that are both serverless and functional and runs using standard Kubernetes building blocks.&lt;br /&gt;&lt;br /&gt;Camel K is the biggest innovation in Apache Camel for the last 10 years. So fill your cup with coffee or tea, and sit back and enjoy the next 10 minutes read.&lt;br /&gt;&lt;br /&gt;I give the floor to Nicola Ferraro (co-creator of Camel K) whom have allowed me to re-post his blog post from the announcement yesterday.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Apache Camel K has made a lot of progress since its inception and we're now proud to announce the 1.0 release. We've been working hard in the past months to add more awesome features to Camel K, but also to improve stability and performance. This post contains a list of cool stuff that you'll find in the 1.0 GA release.&lt;br /&gt;&lt;br /&gt;First of all, if you're living under a rock and it's the first time you hear about Camel K,&lt;br /&gt;you can read some introductory blog posts here (&lt;a href="https://nicolaferraro.me/2018/10/15/introducing-camel-k/"&gt;1 - introducing Camel K&lt;/a&gt;) (&lt;a href="https://www.nicolaferraro.me/2018/12/10/camel-k-on-knative"&gt;2 - camel k on knative&lt;/a&gt;)&lt;br /&gt;or look at the Apache Camel website that contains a &lt;a href="https://camel.apache.org/camel-k/latest/"&gt;Camel K section&lt;/a&gt;&lt;br /&gt;with a lot of material that is automatically generated from the &lt;a href="https://github.com/apache/camel-k"&gt;Github repository&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;User experience&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Camel K development style is minimalistic: you need just to write a single file with your integration routes and you can immediately run them on any Kubernetes cluster. This way of defining things is common to many FaaS platforms (although Camel K is not a proper FaaS platform, but a lightweight integration platform) and it's technically difficult to provide IDE support, such as code completion and other utilities, to developers.&lt;br /&gt;&lt;br /&gt;But now we've it. The integration tooling team has created some cool extensions for VS Code that make the development experience with Camel K even more exciting.&lt;br /&gt;You don't need to remember the Camel DSL syntax, the IDE will give you suggestions and error highlighting.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-K_G7PxgJl_c/XuDSh87YHRI/AAAAAAAACMU/XR9vouL-2dUa98TMnV9stsC8PFyYyLNqACLcBGAsYHQ/s1600/ide-autocompletion.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="563" data-original-width="852" height="263" src="https://1.bp.blogspot.com/-K_G7PxgJl_c/XuDSh87YHRI/AAAAAAAACMU/XR9vouL-2dUa98TMnV9stsC8PFyYyLNqACLcBGAsYHQ/s400/ide-autocompletion.gif" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;Code completion works with Java code, but it's not only limited to it: you also have suggestions and documentation out of the box when writing the Camel URIs and property files.&lt;br /&gt;And you also have many options to run integrations and interact with them, all integrated in the IDE.&lt;br /&gt;&lt;br /&gt;Just install the VS Code &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.apache-camel-extension-pack"&gt;Extension Pack for Apache Camel&lt;/a&gt;&amp;nbsp;to have all these features available.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Getting started tutorials&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Good tools are fundamental to have a great development experience with Camel K, but then you need to learn what you can do with such a great power.&lt;br /&gt;We've created a new repository in the Apache organization that hosts getting started examples: the &lt;a href="https://github.com/apache/camel-k-examples"&gt;camel-k-examples&lt;/a&gt; repository.&lt;br /&gt;&lt;br /&gt;So far we've added guides that drive you through:&lt;br /&gt;&lt;br /&gt;- &lt;a href="https://github.com/apache/camel-k-examples/tree/master/01-basic"&gt;01 Basic&lt;/a&gt;: Learn the basics of Camel K and some interesting use cases&lt;br /&gt;- &lt;a href="https://github.com/apache/camel-k-examples/tree/master/02-serverless-api"&gt;02 Serverless APIs&lt;/a&gt;: How to design a serverless (i.e. auto-scaling, scaling to zero) API and run it in a few minutes&lt;br /&gt;&lt;br /&gt;The basic quickstart is &lt;a href="https://learn.openshift.com/middleware/courses/middleware-camelk/camel-k-basic"&gt;also available online&lt;/a&gt;, so you can have a look at how camel k works &lt;b&gt;without installing anything on your laptop&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;More tutorials are expected to come in the following months. You are also welcome if you want to help us by contributing your own. They are based on the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-didact"&gt;VSCode Didact&lt;/a&gt; project, that provides an&lt;br /&gt;awesome user experience.&lt;br /&gt;&lt;br /&gt;If you are looking for Camel K code samples that you can just pick and run using the CLI, the &lt;a href="https://github.com/apache/camel-k/tree/master/examples"&gt;examples directory&lt;/a&gt;&amp;nbsp;of the Camel K main repository contains a lot of them. You can also run them directly from Github:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run https://raw.githubusercontent.com/apache/camel-k/master/examples/Sample.java&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;You can find ready-to-use examples written in different languages (e.g. &lt;a href="https://github.com/apache/camel-k/blob/5fb589090c2f45b28aef586118df48fad8838b3f/examples/hello.xml"&gt;XML&lt;/a&gt;, &lt;a href="https://github.com/apache/camel-k/blob/5fb589090c2f45b28aef586118df48fad8838b3f/examples/simple.js"&gt;JavaScript&lt;/a&gt; and others).&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Serverless&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Serverless is the most important area where we're focusing the new developments in Apache Camel K, although, you should remember, you can have a wonderful Camel K experience even without serverless features. To enable the serverless profile in Camel K, you just need to have &lt;a href="https://knative.dev/"&gt;Knative&lt;/a&gt; installed.&lt;br /&gt;&lt;br /&gt;In recent releases, we have added support for the most recent advancements in Knative, for example, Camel K is very well integrated with the Knative event broker and you can easily produce or consume events from it.&lt;br /&gt;&lt;br /&gt;With &lt;b&gt;2 lines of code&lt;/b&gt; you can transfer events (e.g. generated by IoT devices) from your MQTT broker to the mesh:&lt;br /&gt;&lt;br /&gt;&lt;i&gt;bridge.groovy&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;from('paho:mytopic?brokerUrl=tcp://broker-address:1883&amp;amp;clientId=knative-bridge')&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&amp;nbsp; .to('knative:event/device-event')&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;No kidding, you just need to write those two lines of code in a file and run it with &lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run bridge.groovy&lt;/span&gt; to push data into the Knative broker.&lt;br /&gt;&lt;br /&gt;And you can also scale the Integration out (Integration is a Kubernetes custom resource, &lt;span style="font-family: Courier New, Courier, monospace;"&gt;kubectl get integrations&lt;/span&gt; to see all of them)&lt;br /&gt;to have a higher throughput. Scaling here is manual because the source of events is a MQTT broker (but we've plans to put &lt;a href="https://github.com/apache/camel-k/issues/1107)"&gt;auto-scaling also in this scenario&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;The Camel K embedded auto-scaling feature works really well when you want to react to some Knative events:&lt;br /&gt;&lt;br /&gt;&lt;i&gt;listener.groovy&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;from('knative:event/device-event')&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&amp;nbsp; .to('http://myhost/webhook/random-id')&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;This integration is configured to receive all events with `type=device-event` and scales automatically with the load because it is materialized into a &lt;a href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0/#service"&gt;Knative Serving Service&lt;/a&gt;&amp;nbsp;and automatically subscribed to the &lt;a href="https://knative.dev/docs/eventing/broker/"&gt;Eventing Broker&lt;/a&gt; via a Trigger.&lt;br /&gt;&lt;br /&gt;It then receives a &lt;a href="https://cloudevents.io/"&gt;CloudEvent&lt;/a&gt; when your IoT devices produce something and scales down to zero if there's no data coming. You just need to create it (as before, just &lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run listener.groovy&lt;/span&gt;), all the remaining configuration is done &lt;b&gt;automatically by the Camel K operator&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;We've added much more features for having a better integration with the Knative ecosystem and we've also fixed some compatibility and performance issues that were present in previous versions. The user experience is now much smoother.&lt;br /&gt;&lt;br /&gt;If you are a Knative YAML developer (!), instead of using Camel K directly, you also have the option to use &lt;a href="https://knative.dev/docs/eventing/samples/apache-camel-source/"&gt;Knative Camel Sources&lt;/a&gt; which are part of the Knative release. They are wrappers for Camel K integrations that are compatible with all the tools used by Knative developers (such as the kn CLI or the OpenShift serverless console).&lt;br /&gt;Sources in Knative can only push data into the various Knative endpoints, but not the other way around (i.e. they cannot be used to publish data from Knative to the outside).&lt;br /&gt;In Camel K you don't have this limitation: the Route is the fundamental building block of a Camel integration and you can do whatever you want with it.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Fast startup and low memory&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;We cannot say we're serverless without mentioning the work that we've been doing in improving the performance of Camel K integrations.&lt;br /&gt;&lt;br /&gt;Starting from Camel 3.3.0 which is the default version used by Camel K 1.0.0, you can benefit from all improvements that have been made directly in the Camel core to make it much more lightweight. More in depth details of the Camel core improvements can be found the following blog series that highlights what has been changed in the 3.x Camel timeline to reduce memory footprint and speedup the startup time, which is fundamental when running integrations in a serverless environment: &lt;a href="http://www.davsclaus.com/2020/01/apache-camel-31-more-camel-core.html?m=1"&gt;part 1&lt;/a&gt;, &lt;a href="http://www.davsclaus.com/2020/01/apache-camel-31-more-camel-core_30.html?m=1"&gt;part 2&lt;/a&gt;&amp;nbsp; &lt;a href="http://www.davsclaus.com/2020/02/apache-camel-31-more-camel-core.html?m=1"&gt;part 3&lt;/a&gt;, &lt;a href="http://www.davsclaus.com/2020/03/apache-camel-32-reflection-free.html?m=1"&gt;part 4&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;But improvements are not only limited to the Camel core: we're doing much more. Several months ago we've started a new subproject of Apache Camel named &lt;a href="https://github.com/apache/camel-quarkus"&gt;Camel Quarkus&lt;/a&gt; with the goal of seamlessly running integrations on top of the Quarkus framework. As you probably know, Quarkus is able to reduce the memory footprint of Java applications and improve the startup time, because it moves much startup logic to the build phase. And Quarkus applications can also be compiled to a native binary, allowing a dramatic improvements in startup performance and very low memory footprint.&lt;br /&gt;&lt;br /&gt;In Camel K 1.0.0 we support Camel Quarkus in JVM mode. A goal is to have also the in-cluster native compilation soon (for some DSL languages, such as YAML), in one of next releases!&lt;br /&gt;&lt;br /&gt;To use Quarkus as underlying runtime, you just need to enable the &lt;a href="https://camel.apache.org/camel-k/latest/traits/quarkus.html"&gt;Quarkus trait&lt;/a&gt; when running an integration:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run myintegration.groovy -t quarkus.enabled=true&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Quarkus is expected to be the default underlying runtime in the next release, and support for Standalone mode (via camel-main) will be deprecated and removed. This means that you won't need to enable Quarkus manually in the next releases, but you still need to do it in 1.0.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Fast build time&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Every application running on Kubernetes needs to be packaged in a container image, but in Camel K you only provide the integration DSL and the operator does what it takes to run it, including building images directly in the cluster.&lt;br /&gt;&lt;br /&gt;The operator manages a pool of reusable container images and if you redeploy your integration code, it does try to reuse existing images from the pool rather than building a new one at each change, because it takes some time to build a new one. It was 1 minute at the beginning...&lt;br /&gt;&lt;br /&gt;But Kubernetes is moving so fast that you cannot solve a problem once and forget about it, you need to take care of it continuously. It happened that some of our third party dependencies that we used for doing builds in "vanilla Kube" has slowly degraded in performance up to a point where Camel K user experience was highly affected.&lt;br /&gt;&lt;br /&gt;We decided to work harder on the build system in order to dramatically improve (again!) the build phase of Camel K integrations.&lt;br /&gt;&lt;br /&gt;Build time can be be now measured in seconds in dev environments such as Minikube. A bunch of seconds, most of the times. This is more than a simple improvement!&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Better CLI&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The 'kamel' CLI is the main tool we provide to developers to run integrations. It's not a mandatory requirement: at the end, an Integration is a Kubernetes custom resources and you can manage it with any Kubernetes standard tool (e.g. kubectl). But the &lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel&lt;/span&gt; CLI adds a lot of value for integration developers.&lt;br /&gt;&lt;br /&gt;For example, if you're a Camel Java developer it's not super easy to remember the boilerplate that you have to write in order to instantiate a Camel route builder. Now you don't have to remember that:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel init Handler.java&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;You get a Java file with all the boilerplate written for you and you just have to write your integration routes.&lt;br /&gt;&lt;br /&gt;It works also with all other languages: Groovy, XML, YAML, Kotlin and JavaScript.&lt;br /&gt;For example you can write:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel init foo.js&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;This way you get a simple route written in JavaScript.&lt;br /&gt;&lt;br /&gt;It's not just that. Often Camel K developers need to add a lot of command line options to configure the final behavior of their integration. For example, you may want to add a custom library with the `-d` option or configure a trait with `-t`. E.g.:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run -d mvn:org.my:lib:1.0.0 -d mvn:org.my:otherlib:2.0.0 -t quarkus.enabled=true Handler.java&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Sometimes the number of command line parameters you've to add can become too many. For this reason we've added the possibility to specify them as modeline options in the integration file (done by adding a comment line with `camel-k:` as prefix).&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Handler.java&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;// camel-k: dependency=mvn:org.my:lib:1.0.0 dependency=mvn:org.my:otherlib:2.0.0 trait=quarkus.enabled=true&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;// ...&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;// your routes here&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Once the options are written in the file, you can run the routes with just:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;// simply this, additional args are read from the file&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run Handler.java&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;The other options are taken automatically from the file modeline. The CLI also displays the full command to let you know what's running.&lt;br /&gt;&lt;br /&gt;This kind of configuration is extremely useful in CI/CD scenarios because it allows you to have self-contained integration files and you don't need to change the pipeline to setup additional options. If you're curious about the CI/CD configurations, you can follow the &lt;a href="https://camel.apache.org/camel-k/latest/tutorials/tekton/tekton.html"&gt;tutorial about Tekton pipelines&lt;/a&gt; to have more information.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Monitoring and Tracing&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Ok, you've finished level 1 of Camel K development and you want to make serious things. You're in a very good position because Camel K provides a lot of useful tools to add visibility on what your integration routes are doing.&lt;br /&gt;&lt;br /&gt;Let's suppose you've a &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt;&amp;nbsp;instance in your namespace and you want to publish your integration metrics:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run Routes.java -t prometheus.enabled=true&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;That's it. No need to setup services a labels to enable scraping. A default prometheus configuration file is also provided for the integration, with sensible defaults. Of course you also have the option to provide &lt;a href="https://camel.apache.org/camel-k/latest/traits/prometheus.html"&gt;your own configuration&lt;/a&gt; for advanced use cases.&lt;br /&gt;&lt;br /&gt;Now, let's suppose you want to see what your routes are doing and trace the execution flow of an integration. What you need to do is to install an opentracing compatible application in the namespace, such as &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt;, and run the integration as:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;kamel run Routes.java -t prometheus.enabled=true -t tracing.enabled=true&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;That's it again. The Camel K operator will add the &lt;a href="https://camel.apache.org/components/latest/others/opentracing.html"&gt;camel-opentracing&lt;/a&gt; library and connect it to the Jaeger collector that is available in the namespace. Here again, &lt;a href="https://camel.apache.org/camel-k/latest/traits/tracing.html"&gt;advanced use cases&lt;/a&gt; are supported.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Master routes&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Good old Camel users know why and when master routes are useful, but for those who are not familiar with the term, I'm going to provide a brief explanation.&lt;br /&gt;&lt;br /&gt;Whenever you have an integration route that must be running, at any point in time, in at most one single Camel instance, you need to use a master route. Master routes can be declared by simply prefixing the consumer endpoint by the 'master' keyword and a name that will be used to create a named lock, e.g.&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;from('master:mylock:telegram:bots')&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&amp;nbsp; .to('log:info')&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;It can be used to print all messages that are sent to your Telegram bot. Since the Telegram API support a single consumer only, you can guard the route with a master prefix to have the guarantee that there will be at most only one consumer at any given time.&lt;br /&gt;&lt;br /&gt;If you're wondering how there can be two instances running of you deploy one, well, think just to when you change your code and need to do a rolling update: for some time there'll be two pods running in parallel. In some cases, you may decide to scale your service out but keep only one instance of a particular route among all the pods of your service. Or you may want to embed a master route in a Knative autoscaling service: in this case, the service can scale autonomously based on the load, but there'll be only one telegram consumer at any time.&lt;br /&gt;&lt;br /&gt;Master routes work out of the box in Camel K, you just need to put a prefix in your endpoint uri. A leader election protocol based on Kubernetes APIs resource locks will be automatically configured for you!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;CronJobs&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;All complex enough systems contain several scheduled jobs. This is especially true for that part of the system that handles integration with the outside.&lt;br /&gt;&lt;br /&gt;Ideally, if you need to execute a quick periodic task, say, every two seconds, you would startup an integration with a route based on timer to execute the periodic task. E.g.&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;Courier New&amp;quot;, Courier, monospace;"&gt;from("timer:task?period=2000")&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&amp;nbsp; .to(this, "businessLogic")&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;But if the period between two executions, instead of 2 seconds ("2000" in the Camel URI, which is measured in milliseconds) is 2 minutes ("120000") or 2 hours ("7200000")?&lt;br /&gt;&lt;br /&gt;You can see that keeping a container with a JVM running for a task that should be executed once every two minutes may be overkill (it is overkill for sure when the period is 2 hours). We live in a time where resources such as memory and CPU are really valuable.&lt;br /&gt;&lt;br /&gt;So the Camel K operator automatically handles this situation by deploying your integration not as a Kubernetes deployment, but as a Kubernetes CronJob. This saves a lot of resources, especially when the period between executions is high. When it's time to run your integration code, a container starts, triggers the execution and then gracefully terminates. Everything is handled automatically by Camel K and Kubernetes.&lt;br /&gt;&lt;br /&gt;There are cases when you don't want this feature to be enabled, for example, when your code makes use of in memory caches that is better to keep between executions. In these cases, you can safely turn off the feature by passing the flag `-t cron.enabled=false` to the `kamel run` command.&lt;br /&gt;&lt;br /&gt;The Cron feature does not only work with the `timer` component. We've also added a &lt;a href="https://camel.apache.org/components/latest/cron-component.html"&gt;cron component&lt;/a&gt; since Camel 3.1 that works really well in combination with the &lt;a href="https://camel.apache.org/camel-k/latest/traits/cron.html"&gt;cron trait&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;So you can also write the cron expression in the route directly:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;from("cron:job?schedule=0/5+*+*+*+?")&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&amp;nbsp; .to(this, "businessLogic")&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;In this case, a new pod with a JVM is started every 5 minutes to execute your scheduled task. For the remaining 4+ minutes you don't use any resource.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Transparency&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Camel K does a lot of work for you when you run your integration code in the cluster and it's possible that you put some errors in the code that can block the deployment process. We've added a lot of visibility on the deployment process that now communicates with the users via Kubernetes events that are printed to the console when you use the CLI.&lt;br /&gt;&lt;br /&gt;This way you're always notified of problems in the code and you can better understand what to fix to make your integration run.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;How to try Camel K 1.0&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The first step is to go to the &lt;a href="https://dist.apache.org/repos/dist/release/camel/camel-k/1.0.0/"&gt;Apache Camel K release page&lt;/a&gt;, download the kamel CLI for your OS and put it in your system path.&lt;br /&gt;&lt;br /&gt;Installation is done usually using the `kamel install` command, but, depending on the kind of Kubernetes cluster you're using, you may need to execute additional configuration steps.&lt;br /&gt;The Camel K documentation contains a section about &lt;a href="https://camel.apache.org/camel-k/latest/installation/installation.html"&gt;installing it on various types of Kubernetes clusters&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;If you have trouble or you need to install it on a particular cluster that is not listed, just reach out in the &lt;a href="https://gitter.im/apache/camel-k"&gt;Gitter chat&lt;/a&gt; and we'll do our best to help you.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Future&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;We've reached version 1.0.0 and this is a great milestone for us. But we are not going to stop now: we've big plans for the future and we'll continue to develop awesome new features.&lt;br /&gt;&lt;br /&gt;We need your help to improve Camel K and we love contributions!&lt;br /&gt;&lt;br /&gt;Join us on:&lt;br /&gt;&lt;br /&gt;- Gitter: &lt;a href="https://gitter.im/apache/camel-k"&gt;https://gitter.im/apache/camel-k&lt;/a&gt;&lt;br /&gt;- GitHub: &lt;a href="https://github.com/apache/camel-k"&gt;https://github.com/apache/camel-k&lt;/a&gt;&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=4Dd5F6Ehusk:_uSXF_I1_VM:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=4Dd5F6Ehusk:_uSXF_I1_VM:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=4Dd5F6Ehusk:_uSXF_I1_VM:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=4Dd5F6Ehusk:_uSXF_I1_VM:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=4Dd5F6Ehusk:_uSXF_I1_VM:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=4Dd5F6Ehusk:_uSXF_I1_VM:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=4Dd5F6Ehusk:_uSXF_I1_VM:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/ApacheCamel/~4/4Dd5F6Ehusk" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9pfCGtn2LMs" height="1" width="1" alt=""/&gt;</content><summary>Yesterday we released Apache Camel K 1.0 and it was announced on social media and on the Camel website. So what is Camel K and why should you care? That is a great question and I want to help answer this by referring to great minds. Hugo Guerrero posted the following tweet   That is a powerful statement from Hugo, where he highlights the groundbreaking innovation from Camel K, that gives developer...</summary><dc:creator>Claus Ibsen</dc:creator><dc:date>2020-06-10T12:51:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/4Dd5F6Ehusk/apache-camel-k-10-is-here-why-should.html</feedburner:origLink></entry><entry><title>Off Heap enhancements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/RBUgma6u_Uw/" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="off-heap" scheme="searchisko:content:tags" /><category term="storage" scheme="searchisko:content:tags" /><author><name>William Burns</name></author><id>searchisko:content:id:jbossorg_blog-off_heap_enhancements</id><updated>2020-06-15T16:48:46Z</updated><published>2020-06-09T12:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The off heap implementation in Infinispan has become much more widely used since its introduction. There have been some issues and improvements identified to get this storage type more in line with its heap counterpart. For those of you that are unware the off-heap setting is actually only "off" the JVM heap and still resides in the native memory of the application.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The best part of all the below changes is the user does not need to change anything, other than configuring Off Heap storage.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_resizing_off_heap_container"&gt;&lt;a class="anchor" href="#_resizing_off_heap_container" /&gt;Resizing Off Heap Container&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;For those of you that have used/configured off heap storage before you may have noticed that there was a configuration option named address count. This setting allowed you to configure how many address pointers the container had. You can think of this as essentially how many buckets you have in a HashMap. Unfortunately the number of pointers was fixed and therefore the user would have to know how many elements they expected to have.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This setting also had another problem. If the user required a larger size of elements this would increase startup time as the container can be iterated upon multiple times when it is empty. Iterating over a container of one million empty pointers would be much slower than iterating over one of only 1024 for example.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;I am glad to say as of Infinispan 10.0.0.Final this setting and the performance of iteration have been greatly improved.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_configuration"&gt;&lt;a class="anchor" href="#_configuration" /&gt;Configuration&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The address count variable is now ignored and instead the off heap based container will start at smaller amount of "buckets" in the range of 128 or 256. We then apply a load factor of .75, which means we will automatically increase the size of the underlying "buckets" once we have inserted a number of entries being 75% or larger than the current "bucket" size.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The resize operation will grow to have double the amount of "buckets" it had prior. The resize operation will be performed concurrently with other operations, providing minimal blocking as we have locks equal to the number of CPUs times two.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This will allow for a cache with off heap to be started significantly faster and relieves some configuration options that were unneeded. Note that the map, just like a java.util.HashMap, will not decrease the number of "buckets" once it grows to a given size.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_iteration_changes"&gt;&lt;a class="anchor" href="#_iteration_changes" /&gt;Iteration changes&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;I mentioned that iteration was slower during startup of larger number of "buckets". This was due to it possibly having a large number of them, however it was also plauged by an ineffecient way of iterating over them. In addition to rewriting the resize operation, we have also optimized the memory layout so that "buckets" can be iterated sequentially which provides more mechanical sympathy.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_hash_changes"&gt;&lt;a class="anchor" href="#_hash_changes" /&gt;Hash changes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This one is rather short and sweet, but the old hash algorithm we used would cause too many collisions for objects that had hash functions that returned values in a similar range, such as java.lang.Integer and java.util.String (with shared startubg characters).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Therefore it has been changed to provide a bit better spreading. This is part of ISPN 10.0.0.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_expiration_bugs"&gt;&lt;a class="anchor" href="#_expiration_bugs" /&gt;Expiration bugs&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Unfortunately off heap had a few issues with expiration. It didn’t support max idle and expiration metadata was not properly transferred to new nodes during state transfer.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In addition to max idle algorithm being rewritten, Off heap now properly supports max idle as of 10.1.4.Final and 11.0.0.Final.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Off heap metadata transferred to new nodes has been fixed in 10.1.8.Final and 11.0.0.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/RBUgma6u_Uw" height="1" width="1" alt=""/&gt;</content><summary>The off heap implementation in Infinispan has become much more widely used since its introduction. There have been some issues and improvements identified to get this storage type more in line with its heap counterpart. For those of you that are unware the off-heap setting is actually only "off" the JVM heap and still resides in the native memory of the application. The best part of all the below ...</summary><dc:creator>William Burns</dc:creator><dc:date>2020-06-09T12:00:00Z</dc:date><feedburner:origLink>http://infinispan.org/blog/2020/06/09/offheap-resize/</feedburner:origLink></entry></feed>
